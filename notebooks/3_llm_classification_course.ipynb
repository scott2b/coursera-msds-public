{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/3_llm_classification_course.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced LLM Classification with vLLM and Unsloth Fine-tuning\n",
    "\n",
    "Welcome to the cutting-edge LLM Classification course! This comprehensive guide covers state-of-the-art techniques for large language model classification using modern tools and methods.\n",
    "\n",
    "## What's Covered in This Course:\n",
    "- **Modern LLM Architectures**: Understanding transformers, attention mechanisms, and scaling laws\n",
    "- **Efficient Inference**: Using vLLM for high-throughput, low-latency inference\n",
    "- **Advanced Fine-tuning**: Parameter-efficient fine-tuning with Unsloth\n",
    "- **Classification Tasks**: Multi-class, multi-label, and hierarchical classification\n",
    "- **Optimization Techniques**: Quantization, pruning, and distillation\n",
    "- **Production Deployment**: Serving LLMs at scale\n",
    "- **Evaluation & Benchmarking**: Comprehensive model evaluation\n",
    "- **Ethical Considerations**: Bias detection and mitigation\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Master modern LLM architectures and training techniques\n",
    "2. Implement efficient inference pipelines with vLLM\n",
    "3. Apply advanced fine-tuning methods with Unsloth\n",
    "4. Deploy and serve LLMs in production environments\n",
    "5. Optimize models for performance and efficiency\n",
    "6. Evaluate and benchmark LLM classification systems\n",
    "7. Address ethical considerations in LLM deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install vllm unsloth transformers datasets accelerate peft trl\n",
    "!pip install bitsandbytes scipy wandb huggingface-hub\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn plotly\n",
    "!pip install fastapi uvicorn gradio\n",
    "\n",
    "# For advanced features\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install auto-gptq optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig, pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Modern LLM Architectures\n",
    "\n",
    "Let's explore the fundamental components of modern large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a modern LLM and examine its architecture\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # Start with a smaller model for demonstration\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model config: {model.config}\")\n",
    "\n",
    "# Examine model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "for name, module in model.named_modules():\n",
    "    if len(name.split('.')) <= 2:  # Only show top-level modules\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Inference with vLLM\n",
    "\n",
    "Learn how to use vLLM for high-throughput, low-latency inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vLLM for efficient inference\n",
    "def setup_vllm(model_name, quantization=\"awq\"):\n",
    "    \"\"\"Setup vLLM with optimized configuration\"\"\"\n",
    "\n",
    "    # vLLM configuration\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        quantization=quantization,\n",
    "        tensor_parallel_size=1,  # Adjust based on available GPUs\n",
    "        gpu_memory_utilization=0.9,\n",
    "        max_model_len=4096,\n",
    "        enforce_eager=False,  # Use CUDA graphs for better performance\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "# For demonstration, we'll use a smaller model that works well with vLLM\n",
    "demo_model = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "try:\n",
    "    print(\"Setting up vLLM...\")\n",
    "    llm = setup_vllm(demo_model, quantization=None)  # No quantization for this demo\n",
    "    print(\"vLLM setup complete!\")\n",
    "\n",
    "    # Test inference\n",
    "    prompts = [\n",
    "        \"Classify this product review as positive or negative: 'This product exceeded my expectations!'\",\n",
    "        \"Analyze the sentiment: 'I'm disappointed with the quality.'\"\n",
    "    ]\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.1,\n",
    "        max_tokens=100,\n",
    "        stop=[\"\\n\", \"\\r\"]\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f\"\\nPrompt {i+1}: {prompts[i]}\")\n",
    "        print(f\"Response: {output.outputs[0].text.strip()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"vLLM setup failed (expected in some environments): {e}\")\n",
    "    print(\"Continuing with standard transformers inference...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Classification\n",
    "\n",
    "Prepare datasets for LLM classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare classification dataset\n",
    "def load_classification_data():\n",
    "    \"\"\"Load a text classification dataset\"\"\"\n",
    "\n",
    "    # Use a standard classification dataset\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:5%]\")  # Small subset for demo\n",
    "\n",
    "    # Convert to binary classification (positive/negative)\n",
    "    def preprocess_function(examples):\n",
    "        return {\n",
    "            \"text\": examples[\"text\"],\n",
    "            \"label\": examples[\"label\"],\n",
    "            \"label_text\": \"positive\" if examples[\"label\"] == 1 else \"negative\"\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(preprocess_function)\n",
    "    return dataset\n",
    "\n",
    "print(\"Loading classification dataset...\")\n",
    "train_dataset = load_classification_data()\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(\"Sample:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Fine-tuning with Unsloth\n",
    "\n",
    "Learn parameter-efficient fine-tuning using Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth setup for efficient fine-tuning\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    print(\"Setting up Unsloth for efficient fine-tuning...\")\n",
    "\n",
    "    # Load model with Unsloth\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\"  # Quantized model for efficiency\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded with Unsloth!\")\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,  # LoRA rank\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,  # Optimized\n",
    "        bias=\"none\",    # Optimized\n",
    "        use_gradient_checkpointing=True,\n",
    "        random_state=3407,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "\n",
    "    print(\"LoRA adapters added!\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Unsloth not available. Using standard PEFT instead...\")\n",
    "\n",
    "    # Fallback to standard transformers + PEFT\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    model_name = \"microsoft/DialoGPT-medium\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Add LoRA\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    print(\"Standard PEFT setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for instruction tuning\n",
    "def format_instruction_example(example):\n",
    "    \"\"\"Format examples for instruction tuning\"\"\"\n",
    "    instruction = \"Classify the sentiment of this movie review as positive or negative.\"\n",
    "\n",
    "    formatted_text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{example['text']}\n",
    "\n",
    "### Response:\n",
    "{example['label_text']}\n",
    "\"\"\"\n",
    "\n",
    "    return {\"text\": formatted_text, \"label\": example[\"label\"]}\n",
    "\n",
    "print(\"Formatting dataset for instruction tuning...\")\n",
    "formatted_dataset = train_dataset.map(format_instruction_example)\n",
    "print(\"Sample formatted example:\")\n",
    "print(formatted_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # Short training for demo\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=50,  # Very short for demo\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    eval_dataset=formatted_dataset.select(range(min(50, len(formatted_dataset)))),\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# trainer.train()  # Commented out for demo\n",
    "print(\"Training completed (simulated)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Optimization\n",
    "\n",
    "Learn advanced model optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model quantization\n",
    "def quantize_model(model, method=\"4bit\"):\n",
    "    \"\"\"Apply quantization to reduce model size and improve inference speed\"\"\"\n",
    "\n",
    "    if method == \"4bit\":\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model.config.name_or_path,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    elif method == \"8bit\":\n",
    "        quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model.config.name_or_path,\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    return quantized_model\n",
    "\n",
    "print(\"Demonstrating model quantization...\")\n",
    "# quantized_model = quantize_model(model, method=\"4bit\")\n",
    "print(\"Quantization example (model size would be reduced by ~75%)\")\n",
    "\n",
    "# Model size comparison\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Original model parameters: {original_params:,}\")\n",
    "print(\"Quantized model would have ~25% of original size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Classification Techniques\n",
    "\n",
    "Implement sophisticated classification approaches with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot and zero-shot classification\n",
    "def setup_classification_pipeline(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"Setup a classification pipeline with modern techniques\"\"\"\n",
    "\n",
    "    # For zero-shot classification, we can use a pipeline\n",
    "    from transformers import pipeline\n",
    "\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"facebook/bart-large-mnli\",\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "    return classifier\n",
    "\n",
    "# Setup classification\n",
    "try:\n",
    "    classifier = setup_classification_pipeline()\n",
    "\n",
    "    # Test classification\n",
    "    test_texts = [\n",
    "        \"This movie was absolutely fantastic! The acting was superb.\",\n",
    "        \"I hated this film. It was boring and poorly made.\",\n",
    "        \"The product arrived quickly and works as expected.\"\n",
    "    ]\n",
    "\n",
    "    candidate_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "    for text in test_texts:\n",
    "        result = classifier(text, candidate_labels)\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Prediction: {result['labels'][0]} (confidence: {result['scores'][0]:.3f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Classification pipeline setup failed: {e}\")\n",
    "    print(\"This is expected in some environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Deployment\n",
    "\n",
    "Learn how to deploy LLMs for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI deployment example\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"LLM Classification API\")\n",
    "\n",
    "# Load model at startup\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global classifier\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"facebook/bart-large-mnli\",\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "class ClassificationRequest(BaseModel):\n",
    "    text: str\n",
    "    labels: list[str]\n",
    "\n",
    "@app.post(\"/classify\")\n",
    "async def classify_text(request: ClassificationRequest):\n",
    "    try:\n",
    "        result = classifier(request.text, request.labels)\n",
    "        return {\n",
    "            \"prediction\": result[\"labels\"][0],\n",
    "            \"confidence\": result[\"scores\"][0],\n",
    "            \"all_scores\": dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save FastAPI code\n",
    "with open('scripts/api_server.py', 'w') as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(\"FastAPI deployment code saved to scripts/api_server.py\")\n",
    "print(\"To run: python scripts/api_server.py\")\n",
    "print(\"API will be available at http://localhost:8000\")\n",
    "print(\"\\nExample request:\")\n",
    "print('curl -X POST \"http://localhost:8000/classify\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d \"{\\\"text\\\": \\\"This product is amazing!\\\", \\\"labels\\\": [\\\"positive\\\", \\\"negative\\\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Benchmarking\n",
    "\n",
    "Comprehensive evaluation techniques for LLM classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_classification_model(predictions, true_labels, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted'\n",
    "    )\n",
    "\n",
    "    # Per-class metrics\n",
    "    per_class = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average=None\n",
    "    )\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for i, (p, r, f) in enumerate(zip(per_class[0], per_class[1], per_class[2])):\n",
    "        print(f\"Class {i}: Precision={p:.4f}, Recall={r:.4f}, F1={f:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Example evaluation with dummy data\n",
    "np.random.seed(42)\n",
    "dummy_predictions = np.random.choice([0, 1], size=100)\n",
    "dummy_true_labels = np.random.choice([0, 1], size=100)\n",
    "\n",
    "results = evaluate_classification_model(\n",
    "    dummy_predictions,\n",
    "    dummy_true_labels,\n",
    "    \"Dummy LLM Classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Considerations and Bias Detection\n",
    "\n",
    "Address ethical issues in LLM classification systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias detection and mitigation\n",
    "def analyze_model_bias(predictions, true_labels, demographic_data):\n",
    "    \"\"\"Analyze potential biases in model predictions\"\"\"\n",
    "\n",
    "    bias_analysis = {}\n",
    "\n",
    "    # Performance by demographic group\n",
    "    for group_name, group_mask in demographic_data.items():\n",
    "        group_predictions = predictions[group_mask]\n",
    "        group_true = true_labels[group_mask]\n",
    "\n",
    "        if len(group_predictions) > 0:\n",
    "            group_accuracy = accuracy_score(group_true, group_predictions)\n",
    "            bias_analysis[group_name] = {\n",
    "                'accuracy': group_accuracy,\n",
    "                'sample_size': len(group_predictions)\n",
    "            }\n",
    "\n",
    "    # Fairness metrics\n",
    "    accuracies = [metrics['accuracy'] for metrics in bias_analysis.values()]\n",
    "    if accuracies:\n",
    "        fairness_score = 1 - (np.std(accuracies) / np.mean(accuracies))\n",
    "        bias_analysis['fairness_score'] = fairness_score\n",
    "\n",
    "    return bias_analysis\n",
    "\n",
    "print(\"Bias analysis framework implemented\")\n",
    "print(\"In practice, you would analyze:\")\n",
    "print(\"- Performance across demographic groups\")\n",
    "print(\"- Fairness metrics (demographic parity, equal opportunity)\")\n",
    "print(\"- Bias mitigation techniques (reweighting, adversarial training)\")\n",
    "print(\"- Regular audits and monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Compare different models and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking framework\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer():\n",
    "    start = time.time()\n",
    "    yield start  # Yield the start time\n",
    "    end = time.time()\n",
    "    print(f\"Time elapsed: {end - start:.2f} seconds\")\n",
    "\n",
    "def benchmark_inference(model, input_texts, batch_sizes=[1, 4, 8]):\n",
    "    \"\"\"Benchmark inference performance\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nBenchmarking batch size {batch_size}...\")\n",
    "\n",
    "        # Prepare batched inputs\n",
    "        batched_texts = [input_texts[i:i+batch_size]\n",
    "                        for i in range(0, len(input_texts), batch_size)]\n",
    "\n",
    "        total_time = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with timer() as t:\n",
    "            for batch in batched_texts:\n",
    "                # Simulate inference\n",
    "                time.sleep(0.1 * len(batch))  # Mock inference time\n",
    "                total_tokens += sum(len(text.split()) for text in batch)\n",
    "\n",
    "        throughput = total_tokens / (time.time() - t)\n",
    "        results[batch_size] = {\n",
    "            'throughput': throughput,\n",
    "            'latency': (time.time() - t) / len(batched_texts)\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example benchmarking\n",
    "sample_texts = [\n",
    "    \"This is a test review for benchmarking.\",\n",
    "    \"Another sample text for performance testing.\",\n",
    "    \"Classification performance evaluation text.\"\n",
    "] * 10  # Repeat for more data\n",
    "\n",
    "print(\"Running inference benchmarks...\")\n",
    "benchmark_results = benchmark_inference(None, sample_texts)\n",
    "\n",
    "for batch_size, metrics in benchmark_results.items():\n",
    "    print(f\"Batch {batch_size}: Throughput = {metrics['throughput']:.2f} tokens/sec, \"\n",
    "          f\"Latency = {metrics['latency']:.3f} sec/batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "Key takeaways from the LLM Classification course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "best_practices = {\n",
    "    \"Model Selection\": [\n",
    "        \"Choose model size based on available resources\",\n",
    "        \"Consider domain-specific pretraining\",\n",
    "        \"Evaluate multiple architectures (Decoder-only, Encoder-Decoder)\"\n",
    "    ],\n",
    "\n",
    "    \"Efficient Inference\": [\n",
    "        \"Use vLLM for high-throughput serving\",\n",
    "        \"Implement quantization (4-bit, 8-bit)\",\n",
    "        \"Batch requests for optimal throughput\",\n",
    "        \"Use model parallelism for large models\"\n",
    "    ],\n",
    "\n",
    "    \"Fine-tuning\": [\n",
    "        \"Use parameter-efficient methods (LoRA, QLoRA)\",\n",
    "        \"Implement proper data formatting\",\n",
    "        \"Monitor for overfitting with validation\",\n",
    "        \"Use gradient checkpointing for memory efficiency\"\n",
    "    ],\n",
    "\n",
    "    \"Production Deployment\": [\n",
    "        \"Implement proper error handling\",\n",
    "        \"Add request rate limiting\",\n",
    "        \"Monitor model performance continuously\",\n",
    "        \"Plan for model updates and A/B testing\"\n",
    "    ],\n",
    "\n",
    "    \"Ethical Considerations\": [\n",
    "        \"Regular bias audits and monitoring\",\n",
    "        \"Implement fairness constraints\",\n",
    "        \"Transparent decision explanations\",\n",
    "        \"Responsible data collection practices\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  \u2022 {practice}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\ud83c\udf89 Congratulations! You've completed the LLM Classification course!\")\n",
    "print(\"You now have the skills to:\")\n",
    "print(\"  \u2022 Build and deploy advanced LLM classification systems\")\n",
    "print(\"  \u2022 Optimize models for production environments\")\n",
    "print(\"  \u2022 Address ethical considerations in AI deployment\")\n",
    "print(\"  \u2022 Implement modern fine-tuning techniques\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}