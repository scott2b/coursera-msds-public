{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/2_llm_fundamentals_theory.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udde0 LLM Fundamentals: Generative AI for Content Analysis\n",
    "\n",
    "## From Textbook Theory to Modern AI Implementation\n",
    "\n",
    "Welcome to the comprehensive LLM Classification course! This notebook bridges foundational generative AI concepts from **\"The Computational Content Analyst\"** with cutting-edge LLM architectures and applications.\n",
    "\n",
    "## \ud83d\udcda Textbook Integration (Chapter 6 + Appendices)\n",
    "\n",
    "### Chapter 6: Leveraging Generative AI for Content Analysis\n",
    "- **LLM Foundations**: Understanding how large language models learn and predict\n",
    "- **Generative Capabilities**: Beyond classification to content generation and analysis\n",
    "- **Content Construction**: How LLMs construct meaning from text data\n",
    "- **Practical Applications**: Real-world content analysis with generative AI\n",
    "- **Research Methodology**: Integrating LLMs into computational content analysis\n",
    "\n",
    "### Appendix A: Codebook and Conceptual Definitions\n",
    "- **Content Categories**: Identity-based, inflammatory, obscene, and threatening language\n",
    "- **Annotation Guidelines**: Systematic coding schemes for content analysis\n",
    "- **Ethical Frameworks**: Responsible classification and content moderation\n",
    "- **Research Standards**: Best practices for reliable content analysis\n",
    "\n",
    "### Appendix B: Content Moderation Guidelines\n",
    "- **Platform Policies**: Understanding content deletion and moderation criteria\n",
    "- **Ethical Boundaries**: Balancing free speech with platform responsibility\n",
    "- **Research Ethics**: Navigating sensitive topics in computational analysis\n",
    "- **Practical Considerations**: Real-world content moderation challenges\n",
    "\n",
    "## \ud83d\ude80 Technical Excellence with Modern Tools\n",
    "\n",
    "### What's New in This Update:\n",
    "- \u2705 **State-of-the-Art Architectures**: Latest LLM models (GPT-4, LLaMA 2, PaLM)\n",
    "- \u2705 **Advanced Optimization**: Quantization, distillation, efficient inference\n",
    "- \u2705 **Production Ready**: Scaling, deployment, and monitoring solutions\n",
    "- \u2705 **Ethical AI**: Bias detection, fairness analysis, responsible deployment\n",
    "- \u2705 **Research Integration**: Connecting textbook methodology with modern AI\n",
    "- \u2705 **Comprehensive Evaluation**: Metrics, benchmarking, and validation\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives (Textbook + Modern AI)\n",
    "\n",
    "### Foundational (From Textbook):\n",
    "1. **Generative AI Theory**: Understand LLM capabilities for content analysis (Chapter 6)\n",
    "2. **Content Coding**: Master systematic annotation and classification schemes\n",
    "3. **Ethical Frameworks**: Apply responsible AI practices in content analysis\n",
    "4. **Research Methodology**: Design studies using generative AI techniques\n",
    "\n",
    "### Technical (Modern Implementation):\n",
    "5. **LLM Architectures**: Master transformer models and modern language models\n",
    "6. **Efficient Inference**: Implement optimized inference pipelines\n",
    "7. **Advanced Techniques**: Fine-tuning, quantization, and model compression\n",
    "8. **Production Systems**: Build scalable, monitored LLM applications\n",
    "9. **Evaluation Methods**: Comprehensive assessment and benchmarking\n",
    "10. **Ethical Deployment**: Responsible AI practices and bias mitigation\n",
    "\n",
    "## \ud83d\udcd6 Textbook-Modern AI Synergy\n",
    "\n",
    "This course uniquely combines:\n",
    "- **\ud83d\udcda Theoretical Foundations** from \"The Computational Content Analyst\"\n",
    "- **\ud83e\udd16 Cutting-edge Technology** with latest LLM architectures\n",
    "- **\u2696\ufe0f Ethical AI** with comprehensive bias detection and fairness\n",
    "- **\ud83c\udfed Production Engineering** with deployment and scaling\n",
    "- **\ud83d\udd2c Research Methodology** connecting academic and industry applications\n",
    "\n",
    "> **Key Insight**: Chapter 6 provides the \"generative AI foundation\" while modern tools provide the \"implementation framework\" - together they create a complete content analysis solution.\n",
    "\n",
    "## \ud83c\udf93 Course Structure\n",
    "\n",
    "### Part 1: Foundations (Textbook Chapter 6)\n",
    "- Generative AI concepts and LLM capabilities\n",
    "- Content analysis with modern language models\n",
    "- Ethical considerations and responsible AI\n",
    "- Research methodology for computational content analysis\n",
    "\n",
    "### Part 2: Modern Implementation (Appendices + SOTA)\n",
    "- Advanced LLM architectures and optimization\n",
    "- Content classification and analysis techniques\n",
    "- Bias detection and fairness analysis\n",
    "- Production deployment and scaling\n",
    "\n",
    "### Part 3: Advanced Applications (Beyond Textbook)\n",
    "- Multi-modal content analysis\n",
    "- Real-time inference optimization\n",
    "- Model interpretability and explainability\n",
    "- Integration with existing workflows\n",
    "\n",
    "## \ud83d\udd2c Key Technical Concepts\n",
    "\n",
    "### From Chapter 6:\n",
    "- **LLM Prediction**: Understanding next-token prediction and sequence modeling\n",
    "- **Content Construction**: How LLMs build meaning from text sequences\n",
    "- **Generative Applications**: Beyond classification to content creation\n",
    "- **Research Integration**: Connecting LLMs with content analysis methodology\n",
    "\n",
    "### From Appendices:\n",
    "- **Content Categories**: Systematic classification of language types\n",
    "- **Ethical Boundaries**: Platform policies and moderation guidelines\n",
    "- **Annotation Standards**: Reliable coding schemes and definitions\n",
    "- **Research Ethics**: Navigating sensitive content in analysis\n",
    "\n",
    "## \ud83c\udfc6 Success Metrics\n",
    "\n",
    "By the end of this course, you will be able to:\n",
    "- \u2705 Apply generative AI for content analysis (Chapter 6 methodology)\n",
    "- \u2705 Implement systematic content coding using textbook frameworks\n",
    "- \u2705 Build ethical LLM applications with bias detection\n",
    "- \u2705 Deploy production-ready content analysis systems\n",
    "- \u2705 Connect academic research with modern AI implementation\n",
    "- \u2705 Navigate ethical challenges in computational content analysis\n",
    "\n",
    "## \ud83d\udcca Industry Applications\n",
    "\n",
    "### Content Analysis Research:\n",
    "- **Social Media Monitoring**: Automated content classification and analysis\n",
    "- **News Analysis**: AI-powered news categorization and sentiment analysis\n",
    "- **Academic Research**: Large-scale content analysis with LLMs\n",
    "- **Policy Analysis**: Automated policy document analysis and classification\n",
    "\n",
    "### Business Applications:\n",
    "- **Content Moderation**: Automated detection of harmful content\n",
    "- **Brand Monitoring**: Real-time brand mention analysis and classification\n",
    "- **Customer Service**: Automated ticket classification and routing\n",
    "- **Market Research**: Consumer feedback analysis and trend identification\n",
    "\n",
    "## \u2696\ufe0f Ethical AI Framework\n",
    "\n",
    "### From Textbook Appendices:\n",
    "- **Content Categories**: Identity-based, inflammatory, obscene, threatening language\n",
    "- **Moderation Guidelines**: Platform policies and deletion criteria\n",
    "- **Research Ethics**: Handling sensitive topics responsibly\n",
    "- **Bias Mitigation**: Systematic approaches to fair content analysis\n",
    "\n",
    "### Modern Implementation:\n",
    "- **Bias Detection**: Automated identification of discriminatory content\n",
    "- **Fairness Analysis**: Ensuring equitable treatment across user groups\n",
    "- **Transparency**: Explainable AI decisions and model interpretability\n",
    "- **Accountability**: Auditing and monitoring AI system behavior\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to leverage generative AI for content analysis? Let's begin with LLM fundamentals! \ud83d\ude80**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (cross-platform)\n",
    "import sys, platform, os, subprocess\n",
    "\n",
    "def pipi(args): subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
    "\n",
    "gpu_linux = platform.system() == \"Linux\" and os.path.exists(\"/proc/driver/nvidia/version\")\n",
    "if gpu_linux:\n",
    "    pipi([\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu121\"])\n",
    "else:\n",
    "    pipi([\"torch\", \"torchvision\", \"torchaudio\"])\n",
    "\n",
    "pipi([\"transformers>=4.43\", \"datasets\", \"accelerate>=0.33\", \"torchinfo\", \"plotly\", \"matplotlib\", \"seaborn\", \"bertviz\"])\n",
    "\n",
    "if gpu_linux:\n",
    "    pipi([\"bitsandbytes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
    "    GPT2Tokenizer, GPT2Model, GPT2LMHeadModel,\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from torchinfo import summary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\ud83d\ude80 LLM Fundamentals Environment Ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Evolution of Language Models\n",
    "\n",
    "Let's explore how language models have evolved over time and understand the key breakthroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution timeline of language models\n",
    "lm_evolution = {\n",
    "    2013: {\"name\": \"Word2Vec\", \"params\": \"~300M\", \"architecture\": \"Shallow NN\"},\n",
    "    2014: {\"name\": \"GloVe\", \"params\": \"~400M\", \"architecture\": \"Matrix Factorization\"},\n",
    "    2015: {\"name\": \"LSTM Language Models\", \"params\": \"~10M\", \"architecture\": \"RNN\"},\n",
    "    2017: {\"name\": \"Transformer\", \"params\": \"~65M\", \"architecture\": \"Attention\"},\n",
    "    2018: {\"name\": \"BERT\", \"params\": \"~340M\", \"architecture\": \"Bidirectional Transformer\"},\n",
    "    2019: {\"name\": \"GPT-2\", \"params\": \"~1.5B\", \"architecture\": \"Autoregressive Transformer\"},\n",
    "    2020: {\"name\": \"T5\", \"params\": \"~11B\", \"architecture\": \"Encoder-Decoder\"},\n",
    "    2022: {\"name\": \"PaLM\", \"params\": \"~540B\", \"architecture\": \"Dense Transformer\"},\n",
    "    2023: {\"name\": \"LLaMA 2\", \"params\": \"~70B\", \"architecture\": \"Optimized Transformer\"},\n",
    "    2023: {\"name\": \"GPT-4\", \"params\": \"~1.7T\", \"architecture\": \"Mixture of Experts\"},\n",
    "    2024: {\"name\": \"Grok-1\", \"params\": \"~314B\", \"architecture\": \"Mixture of Experts\"}\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "years = list(lm_evolution.keys())\n",
    "names = [lm_evolution[year][\"name\"] for year in years]\n",
    "params = [lm_evolution[year][\"params\"] for year in years]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=years,\n",
    "    y=list(range(len(years))),\n",
    "    mode='lines+markers+text',\n",
    "    text=names,\n",
    "    textposition=\"top center\",\n",
    "    name='Model Evolution'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Evolution of Language Models (2013-2024)',\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Progression',\n",
    "    showlegend=False,\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Display evolution table\n",
    "import pandas as pd\n",
    "df_evolution = pd.DataFrame.from_dict(lm_evolution, orient='index')\n",
    "df_evolution.index.name = 'Year'\n",
    "print(\"\\nLanguage Model Evolution:\")\n",
    "print(df_evolution.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Transformer Architecture Deep Dive\n",
    "\n",
    "Let's examine the core components of transformer architecture that powers all modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small transformer model to examine its architecture\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "\n",
    "# Add pad token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"GPT-2 Model Architecture:\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Number of attention heads: {model.config.n_head}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")\n",
    "\n",
    "# Examine model structure\n",
    "print(\"\\nModel Structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if len(name.split('.')) <= 2 and name:  # Show top-level modules\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed model summary\n",
    "print(\"\\nDetailed Model Summary:\")\n",
    "print(summary(model, input_size=(1, 10), dtypes=[torch.long]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Attention Mechanism Deep Dive\n",
    "\n",
    "The attention mechanism is the heart of transformer models. Let's understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple attention mechanism from scratch\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = embed_dim ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        Q = self.query(x)  # (batch, seq, embed)\n",
    "        K = self.key(x)    # (batch, seq, embed)\n",
    "        V = self.value(x)  # (batch, seq, embed)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply attention\n",
    "        attended = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return attended, attention_weights\n",
    "\n",
    "# Test the attention mechanism\n",
    "attention = SimpleAttention(embed_dim=64)\n",
    "test_input = torch.randn(2, 10, 64)  # batch=2, seq=10, embed=64\n",
    "output, weights = attention(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(weights[0].detach().numpy(), cmap='viridis')\n",
    "plt.title('Attention Weights Visualization')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd2c Comparing Modern LLM Architectures\n",
    "\n",
    "Let's compare different LLM architectures and their strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model architectures\n",
    "architectures = {\n",
    "    'GPT-style (Autoregressive)': {\n",
    "        'strengths': ['Strong text generation', 'Efficient inference', 'Good at creative tasks'],\n",
    "        'weaknesses': ['Unidirectional', 'Limited understanding of full context'],\n",
    "        'use_cases': ['Content generation', 'Code completion', 'Chatbots'],\n",
    "        'examples': ['GPT-3', 'GPT-4', 'LLaMA']\n",
    "    },\n",
    "    'BERT-style (Bidirectional)': {\n",
    "        'strengths': ['Strong understanding', 'Good at classification', 'Context-aware'],\n",
    "        'weaknesses': ['Complex training', 'Slower inference', 'Less creative'],\n",
    "        'use_cases': ['Sentiment analysis', 'Question answering', 'Named entity recognition'],\n",
    "        'examples': ['BERT', 'RoBERTa', 'ALBERT']\n",
    "    },\n",
    "    'T5-style (Encoder-Decoder)': {\n",
    "        'strengths': ['Versatile', 'Good at translation', 'Structured generation'],\n",
    "        'weaknesses': ['Complex architecture', 'Higher computational cost'],\n",
    "        'use_cases': ['Translation', 'Summarization', 'Question answering'],\n",
    "        'examples': ['T5', 'BART', 'Pegasus']\n",
    "    },\n",
    "    'Mixture of Experts (MoE)': {\n",
    "        'strengths': ['Scalable', 'Efficient routing', 'High performance'],\n",
    "        'weaknesses': ['Complex routing', 'Higher latency', 'Training challenges'],\n",
    "        'use_cases': ['Large-scale applications', 'Multilingual tasks'],\n",
    "        'examples': ['Grok-1', 'Switch Transformer', 'GShard']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for arch, details in architectures.items():\n",
    "    comparison_data.append({\n",
    "        'Architecture': arch,\n",
    "        'Strengths': ', '.join(details['strengths'][:2]),\n",
    "        'Use Cases': ', '.join(details['use_cases'][:2]),\n",
    "        'Examples': ', '.join(details['examples'])\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"LLM Architecture Comparison:\")\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Scaling Laws and Model Sizes\n",
    "\n",
    "Understanding how model performance scales with size and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling laws visualization\n",
    "model_sizes = [0.1, 0.3, 1, 3, 7, 13, 30, 65, 175, 540]  # Billions of parameters\n",
    "performance = [65, 70, 75, 78, 80, 82, 84, 86, 88, 90]  # Approximate performance scores\n",
    "compute_cost = [10**x for x in range(len(model_sizes))]  # Relative compute cost\n",
    "\n",
    "# Create scaling plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance vs Model Size\n",
    "ax1.plot(model_sizes, performance, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Model Size (Billions of Parameters)')\n",
    "ax1.set_ylabel('Performance Score')\n",
    "ax1.set_title('Scaling Laws: Performance vs Model Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Performance vs Compute Cost\n",
    "ax2.plot(compute_cost, performance, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Compute Cost (Relative)')\n",
    "ax2.set_ylabel('Performance Score')\n",
    "ax2.set_title('Scaling Laws: Performance vs Compute')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key scaling insights\n",
    "print(\"\\n\ud83d\udd11 Key Scaling Law Insights:\")\n",
    "print(\"1. Performance scales as power law with model size\")\n",
    "print(\"2. Larger models are more sample-efficient\")\n",
    "print(\"3. Cross-entropy loss decreases predictably with scale\")\n",
    "print(\"4. Compute-optimal models balance size and training time\")\n",
    "print(\"5. Scaling laws hold across different architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udddc\ufe0f Quantization and Model Compression\n",
    "\n",
    "Learn how to reduce model size while maintaining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quantization techniques\n",
    "def demonstrate_quantization():\n",
    "    \"\"\"Show different quantization approaches\"\"\"\n",
    "\n",
    "    # Original model\n",
    "    model_fp32 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "    original_size = sum(p.numel() * p.element_size() for p in model_fp32.parameters())\n",
    "\n",
    "    print(f\"Original model size: {original_size / (1024**2):.2f} MB\")\n",
    "\n",
    "    # Simulate different quantization levels\n",
    "    quantization_levels = {\n",
    "        'FP32': 32,\n",
    "        'FP16': 16,\n",
    "        'INT8': 8,\n",
    "        'INT4': 4,\n",
    "        'INT2': 2,\n",
    "        'INT1': 1\n",
    "    }\n",
    "\n",
    "    print(\"\\nQuantization Comparison:\")\n",
    "    print(\"Level  | Bits | Size Reduction | Memory Savings\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    for level, bits in quantization_levels.items():\n",
    "        size_reduction = 32 / bits\n",
    "        memory_savings = (1 - 1/size_reduction) * 100\n",
    "        quantized_size = original_size / size_reduction\n",
    "        print(f\"{level:6} | {bits:4} | {size_reduction:2.1f}x          | {memory_savings:5.1f}%\")\n",
    "        print(f\"       |      | {quantized_size/(1024**2):6.2f} MB     |\")\n",
    "        print()\n",
    "\n",
    "demonstrate_quantization()\n",
    "\n",
    "# Load a quantized model\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Loading quantized model...\")\n",
    "try:\n",
    "    quantized_model = GPT2Model.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"\u2705 Quantized model loaded successfully!\")\n",
    "    print(f\"Model device: {next(quantized_model.parameters()).device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Quantization demo requires compatible hardware: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfa8 Tokenization and Embeddings\n",
    "\n",
    "Understanding how text is converted to numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore tokenization in depth\n",
    "text = \"Hello, I'm learning about Large Language Models! This is fascinating. \ud83e\udd16\"\n",
    "\n",
    "# Compare different tokenizers\n",
    "tokenizers = {\n",
    "    'GPT-2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    'BERT': BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "}\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    print(f\"\\n\ud83d\udd24 {name} Tokenization:\")\n",
    "    print(f\"Original text: {text}\")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "    # Decode back\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Visualize tokenization differences\n",
    "gpt2_tokens = tokenizers['GPT-2'].tokenize(text)\n",
    "bert_tokens = tokenizers['BERT'].tokenize(text)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Tokenization Comparison:\")\n",
    "print(f\"GPT-2 tokens: {len(gpt2_tokens)} - {gpt2_tokens}\")\n",
    "print(f\"BERT tokens: {len(bert_tokens)} - {bert_tokens}\")\n",
    "print(f\"Difference: {len(gpt2_tokens) - len(bert_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore embeddings\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get embeddings for a simple sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(f\"Input sentence: {sentence}\")\n",
    "print(f\"Tokenized: {tokenizer.tokenize(sentence)}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[-1]}\")\n",
    "\n",
    "# Visualize embedding similarities\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_embeddings = embeddings[0]  # Remove batch dimension\n",
    "\n",
    "# Calculate cosine similarities between token embeddings\n",
    "cosine_similarities = torch.nn.functional.cosine_similarity(\n",
    "    token_embeddings.unsqueeze(1),\n",
    "    token_embeddings.unsqueeze(0),\n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cosine_similarities.numpy(),\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='coolwarm',\n",
    "            center=0)\n",
    "plt.title('Token Embedding Similarities')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Hands-on Exercise: Building a Simple Transformer\n",
    "\n",
    "Let's build a simplified transformer from scratch to understand the internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"A simplified transformer block\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Test the transformer block\n",
    "transformer = SimpleTransformerBlock(embed_dim=64, num_heads=4, ff_dim=128)\n",
    "test_input = torch.randn(2, 10, 64)  # batch=2, seq=10, embed=64\n",
    "output = transformer(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Transformer block created successfully!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Key Takeaways\n",
    "\n",
    "1. **Evolution**: LLMs have evolved from simple word embeddings to massive transformer models\n",
    "2. **Architecture**: All modern LLMs are based on the transformer architecture\n",
    "3. **Attention**: The attention mechanism is the key innovation enabling long-range dependencies\n",
    "4. **Scaling**: Model performance follows predictable scaling laws\n",
    "5. **Quantization**: Model compression techniques enable deployment on resource-constrained devices\n",
    "6. **Tokenization**: Text preprocessing is crucial for model performance\n",
    "7. **Embeddings**: Dense vector representations capture semantic meaning\n",
    "\n",
    "## \ud83d\ude80 Next Steps\n",
    "\n",
    "Now that you understand LLM fundamentals, proceed to:\n",
    "- **Notebook 2**: Efficient Inference with vLLM\n",
    "- **Notebook 3**: Advanced Fine-tuning with Unsloth\n",
    "- **Notebook 4**: Production Deployment and Scaling\n",
    "- **Notebook 5**: Evaluation, Benchmarking, and Ethics\n",
    "\n",
    "## \ud83d\udd17 Additional Resources\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original transformer paper\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3 paper\n",
    "- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "## \ud83c\udfaf Quiz Time!\n",
    "\n",
    "Test your understanding of LLM fundamentals:\n",
    "1. What are the three main components of a transformer block?\n",
    "2. How does attention differ from traditional RNN approaches?\n",
    "3. What are the trade-offs between different quantization levels?\n",
    "4. Why do larger models generally perform better?\n",
    "5. What role does tokenization play in LLM performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}