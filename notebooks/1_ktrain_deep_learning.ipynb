{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/1_ktrain_deep_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification of News Media Content Categories with Deep Learning\n",
    "\n",
    "In this notebook, we will build a deep learning model to classify news articles into content categories, with a focus on identifying articles related to health and wellness. This task is crucial for contextual advertising, where ads are displayed alongside content relevant to their target audience.\n",
    "\n",
    "## Objectives:\n",
    "- Load and preprocess a dataset of news headlines for classification.\n",
    "- Address class imbalance in the dataset.\n",
    "- Utilize a pretrained BERT model for text classification.\n",
    "- Train and validate the deep learning model.\n",
    "- Interpret the model's predictions using SHAP values.\n",
    "- Discuss the implications of using probabilities vs binary classifications.\n",
    "\n",
    "## Key Points:\n",
    "- Handling imbalanced datasets to prevent model bias.\n",
    "- Using pretrained word embeddings from BERT for nuanced text understanding.\n",
    "- Fine-tuning a deep learning model with TensorFlow and Keras.\n",
    "- Evaluating model performance with precision, recall, and F1-score.\n",
    "- Model interpretability with SHAP values.\n",
    "\n",
    "## Prerequisites:\n",
    "- Familiarity with Python programming.\n",
    "- Basic understanding of machine learning and deep learning concepts.\n",
    "- Experience with pandas, TensorFlow, and Keras libraries.\n",
    "\n",
    "Let's begin by setting up our environment and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "The data file for this lab is available in the Coursera lab environment in the `data/` directory. To use it in Colab:\n",
    "\n",
    "1. In the Coursera lab, click the **Files** tab and navigate to `lab2 / data`\n",
    "2. Select the checkbox next to `News_Category_Dataset_v3.json`, then click the **Download** button in the toolbar\n",
    "3. In your Google Drive, create a folder called `coursera-msds` (if you haven't already)\n",
    "4. Upload the downloaded file to `coursera-msds/`\n",
    "\n",
    "The code below will mount your Google Drive and load the data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "try:\n",
    "    from transformers import TFAutoModel  # removed in transformers v5\n",
    "    import ktrain\n",
    "    from ktrain import text\n",
    "    import shap\n",
    "except ImportError:\n",
    "    # ktrain needs TF model classes from transformers v4.x\n",
    "    !pip uninstall -y transformers\n",
    "    !pip install \"transformers>=4.43,<5\" \"numpy<2\" ktrain shap tf-keras eli5\n",
    "    os.kill(os.getpid(), 9)  # restart runtime; cell re-runs cleanly\n",
    "\n",
    "import numpy as np\n",
    "np.Inf = np.inf  # compatibility shim: np.Inf removed in NumPy 2.0\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset source: https://www.kaggle.com/datasets/rmisra/news-category-dataset\n",
    "DATA_FILENAME = 'News_Category_Dataset_v3.json'\n",
    "DRIVE_PATH = f'/content/drive/MyDrive/coursera-msds/{DATA_FILENAME}'\n",
    "LOCAL_PATH = f'data/{DATA_FILENAME}'\n",
    "\n",
    "if os.path.exists(DRIVE_PATH):\n",
    "    data_path = DRIVE_PATH\n",
    "    print(f'Loading data from Google Drive: {DRIVE_PATH}')\n",
    "elif os.path.exists(LOCAL_PATH):\n",
    "    data_path = LOCAL_PATH\n",
    "    print(f'Loading data from local path: {LOCAL_PATH}')\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f'Data file not found. Please either:\\n'\n",
    "        f'  1. Upload {DATA_FILENAME} to Google Drive at: MyDrive/coursera-msds/\\n'\n",
    "        f'  2. Place {DATA_FILENAME} in a local data/ directory'\n",
    "    )\n",
    "\n",
    "reviews = pd.read_json(data_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's preprocess the data by combining the 'headline' and 'short_description' columns and creating a binary 'healthy' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine headline and short_description into one column\n",
    "reviews['combined_text'] = reviews['headline'] + ' ' + reviews['short_description']\n",
    "\n",
    "# Create a binary label for health-related articles\n",
    "reviews['healthy'] = np.where((reviews['category'] == 'HEALTHY LIVING'), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address class imbalance, we will resample the dataset to have equal representation of both classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this lab, we use a smaller balanced sample to keep training\n",
    "# times manageable. For your final project, consider using the full\n",
    "# balanced set: sample_amount = len(reviews[reviews['healthy'] == 1])\n",
    "sample_amount = 5000\n",
    "\n",
    "healthy = reviews[reviews['healthy'] == 1].sample(n=sample_amount)\n",
    "not_healthy = reviews[reviews['healthy'] == 0].sample(n=sample_amount)\n",
    "review_sample = pd.concat([healthy, not_healthy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the data for the BERT model, setting a maximum token length and splitting the data into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT model with ktrain\n",
    "# maxlen=128 is sufficient: mean headline+description is ~29 tokens,\n",
    "# 99th percentile is ~71. Using 512 wastes GPU memory.\n",
    "train, val, preprocess = ktrain.text.texts_from_df(\n",
    "    review_sample,\n",
    "    'combined_text',\n",
    "    label_columns=['healthy'],\n",
    "    val_df=None,\n",
    "    max_features=20000,\n",
    "    maxlen=128,\n",
    "    val_pct=0.1,\n",
    "    preprocess_mode='distilbert',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model using the `autofit` method, which includes early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learner object with the preprocessed data and model\n",
    "learner = ktrain.get_learner(\n",
    "    preprocess.get_classifier(),\n",
    "    train_data=train,\n",
    "    val_data=val,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = learner.autofit(\n",
    "    1e-4,\n",
    "    checkpoint_folder='checkpoint',\n",
    "    epochs=10,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we evaluate the model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance\n",
    "validation = learner.validate(val_data=val, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can interpret the model's predictions using SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc=t)\n",
    "# Example article for interpretation\n",
    "article = \"Boulder, CO – The University of Colorado, Boulder’s own Professor Pat Ferrucci was honored this week with a prestigious award recognizing him as the top fitness enthusiast on campus.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHAP values will help us understand the contribution of each word to the classification decision.\n",
    "\n",
    "In conclusion, we have successfully built and evaluated a deep learning model for text classification, with a focus on identifying health and wellness content for contextual advertising. The model's interpretability was enhanced using SHAP values, providing insights into the decision-making process. The findings from this notebook can be leveraged to improve targeted advertising strategies and contribute to more effective marketing campaigns."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
