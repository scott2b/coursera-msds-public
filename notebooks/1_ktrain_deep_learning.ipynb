{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/1_ktrain_deep_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Classification of News Media Content Categories with Deep Learning\n",
    "\n",
    "In this notebook, we will build a deep learning model to classify news articles into content categories, with a focus on identifying articles related to health and wellness. This task is crucial for contextual advertising, where ads are displayed alongside content relevant to their target audience.\n",
    "\n",
    "## Objectives:\n",
    "- Load and preprocess a dataset of news headlines for classification.\n",
    "- Address class imbalance in the dataset.\n",
    "- Utilize a pretrained BERT model for text classification.\n",
    "- Train and validate the deep learning model.\n",
    "- Interpret the model's predictions using SHAP values.\n",
    "- Discuss the implications of using probabilities vs binary classifications.\n",
    "\n",
    "## Key Points:\n",
    "- Handling imbalanced datasets to prevent model bias.\n",
    "- Using pretrained word embeddings from BERT for nuanced text understanding.\n",
    "- Fine-tuning a deep learning model with TensorFlow and Keras.\n",
    "- Evaluating model performance with precision, recall, and F1-score.\n",
    "- Model interpretability with SHAP values.\n",
    "\n",
    "## Prerequisites:\n",
    "- Familiarity with Python programming.\n",
    "- Basic understanding of machine learning and deep learning concepts.\n",
    "- Experience with pandas, TensorFlow, and Keras libraries.\n",
    "\n",
    "Let's begin by setting up our environment and importing the necessary libraries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "The data file for this lab is available in the Coursera lab environment in the `data/` directory. To use it in Colab:\n",
    "\n",
    "1. In the Coursera lab, click the **Files** tab and navigate to `lab2 / data`\n",
    "2. Select the checkbox next to `News_Category_Dataset_v3.json`, then click the **Download** button in the toolbar\n",
    "3. In your Google Drive, create a folder called `coursera-msds` (if you haven't already)\n",
    "4. Upload the downloaded file to `coursera-msds/`\n",
    "\n",
    "The code below will mount your Google Drive and load the data automatically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ktrain requires TensorFlow-based model classes (TFAutoModel) which\n",
    "# were removed in transformers v5. We must uninstall the Colab default\n",
    "# and install a v4.x release.\n",
    "!pip uninstall -y transformers\n",
    "!pip install \"transformers>=4.43,<5\" ktrain shap tf-keras eli5\n",
    "\n",
    "# Verify the correct version was installed\n",
    "import transformers\n",
    "print(f'transformers version: {transformers.__version__}')\n",
    "assert int(transformers.__version__.split('.')[0]) == 4, (\n",
    "    f'Expected transformers v4.x but got {transformers.__version__}'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Restart the runtime so all packages load cleanly.\n",
    "# After restart, skip the two cells above and continue from below.\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **After the runtime restarts**, skip the install and restart cells above and continue running from here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import shap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dataset source: https://www.kaggle.com/datasets/rmisra/news-category-dataset\n",
    "DATA_FILENAME = 'News_Category_Dataset_v3.json'\n",
    "DRIVE_PATH = f'/content/drive/MyDrive/coursera-msds/{DATA_FILENAME}'\n",
    "LOCAL_PATH = f'data/{DATA_FILENAME}'\n",
    "\n",
    "if os.path.exists(DRIVE_PATH):\n",
    "    data_path = DRIVE_PATH\n",
    "    print(f'Loading data from Google Drive: {DRIVE_PATH}')\n",
    "elif os.path.exists(LOCAL_PATH):\n",
    "    data_path = LOCAL_PATH\n",
    "    print(f'Loading data from local path: {LOCAL_PATH}')\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f'Data file not found. Please either:\\n'\n",
    "        f'  1. Upload {DATA_FILENAME} to Google Drive at: MyDrive/coursera-msds/\\n'\n",
    "        f'  2. Place {DATA_FILENAME} in a local data/ directory'\n",
    "    )\n",
    "\n",
    "reviews = pd.read_json(data_path, lines=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's preprocess the data by combining the 'headline' and 'short_description' columns and creating a binary 'healthy' label."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Combine headline and short_description into one column\n",
    "reviews['combined_text'] = reviews['headline'] + ' ' + reviews['short_description']\n",
    "\n",
    "# Create a binary label for health-related articles\n",
    "reviews['healthy'] = np.where((reviews['category'] == 'HEALTHY LIVING'), 1, 0)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To address class imbalance, we will resample the dataset to have equal representation of both classes.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Balance the dataset by sampling equal instances of both classes\n",
    "healthy = reviews[reviews['healthy'] == 1]\n",
    "sample_amount = len(healthy)\n",
    "not_healthy = reviews[reviews['healthy'] == 0].sample(n=sample_amount)\n",
    "review_sample = pd.concat([healthy, not_healthy])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we prepare the data for the BERT model, setting a maximum token length and splitting the data into training and validation sets.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the BERT model with ktrain\n",
    "t = text.Transformer('distilbert-base-uncased', maxlen=512, class_names=['healthy', 'not_healthy'])\n",
    "\n",
    "# Preprocess the text data and split into training and validation sets\n",
    "train, val, preprocess = text.texts_from_df(\n",
    "    review_sample,\n",
    "    'combined_text',\n",
    "    label_columns=['healthy'],\n",
    "    val_df=None,\n",
    "    max_features=20000,\n",
    "    maxlen=512,\n",
    "    val_pct=0.1,\n",
    "    preprocess_mode='distilbert',\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now train the model using the `autofit` method, which includes early stopping to prevent overfitting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a learner object with the preprocessed data and model\n",
    "learner = ktrain.get_learner(t.get_classifier(), train_data=train, val_data=val, batch_size=6)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = learner.autofit(1e-4, checkpoint_folder='checkpoint', epochs=12, early_stopping=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, we evaluate the model's performance on the validation set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate the model's performance\n",
    "validation = learner.validate(val_data=val, print_report=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can interpret the model's predictions using SHAP values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc=t)\n",
    "# Example article for interpretation\n",
    "article = \"Boulder, CO \u2013 The University of Colorado, Boulder\u2019s own Professor Pat Ferrucci was honored this week with a prestigious award recognizing him as the top fitness enthusiast on campus.\"\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictor.explain(article)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The SHAP values will help us understand the contribution of each word to the classification decision.\n",
    "\n",
    "In conclusion, we have successfully built and evaluated a deep learning model for text classification, with a focus on identifying health and wellness content for contextual advertising. The model's interpretability was enhanced using SHAP values, providing insights into the decision-making process. The findings from this notebook can be leveraged to improve targeted advertising strategies and contribute to more effective marketing campaigns."
   ],
   "metadata": {}
  }
 ]
}