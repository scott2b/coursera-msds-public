{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/4_efficient_inference_vllm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u26a1 Efficient Inference with vLLM\n",
    "\n",
    "This notebook dives deep into high-performance LLM inference using vLLM, covering everything from basic setup to advanced optimization techniques.\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Master vLLM installation and setup\n",
    "2. Understand different inference optimization techniques\n",
    "3. Implement batched inference for maximum throughput\n",
    "4. Learn about continuous batching and memory management\n",
    "5. Compare vLLM with other inference engines\n",
    "6. Deploy models with different quantization levels\n",
    "7. Monitor and profile inference performance\n",
    "8. Handle production workloads and scaling\n",
    "\n",
    "## \ud83d\udd27 Prerequisites\n",
    "\n",
    "- Completed Notebook 1 (LLM Fundamentals)\n",
    "- CUDA-compatible GPU (recommended)\n",
    "- Basic understanding of model serving\n",
    "- Familiarity with performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM and dependencies\n",
    "!pip install vllm\n",
    "!pip install ray  # For distributed inference\n",
    "!pip install transformers accelerate\n",
    "!pip install psutil GPUtil  # For monitoring\n",
    "!pip install matplotlib seaborn plotly\n",
    "\n",
    "# Optional: Install for advanced quantization\n",
    "# !pip install auto-gptq optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\ud83d\ude80 vLLM Inference Environment Ready!\")\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check available hardware\n",
    "print(f\"\\n\ud83d\udda5\ufe0f  CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"\ud83e\udde0 RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "try:\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        gpu = gpus[0]\n",
    "        print(f\"\ud83c\udfae GPU: {gpu.name}\")\n",
    "        print(f\"\ud83e\udde0 GPU Memory: {gpu.memoryTotal} MB\")\n",
    "        print(f\"\ud83d\udcca GPU Utilization: {gpu.load * 100:.1f}%\")\n",
    "    else:\n",
    "        print(\"\u274c No GPU detected\")\n",
    "except:\n",
    "    print(\"\u26a0\ufe0f  GPU monitoring not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f vLLM Architecture and Setup\n",
    "\n",
    "Understanding vLLM's core components and setup process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vllm_model(model_name: str, quantization: str = None, **kwargs):\n",
    "    \"\"\"Setup vLLM model with optimal configuration\"\"\"\n",
    "\n",
    "    print(f\"\ud83d\ude80 Setting up vLLM with {model_name}...\")\n",
    "\n",
    "    # Default configuration\n",
    "    config = {\n",
    "        \"model\": model_name,\n",
    "        \"tensor_parallel_size\": 1,  # Increase for multi-GPU\n",
    "        \"gpu_memory_utilization\": 0.85, # Reduced GPU memory utilization\n",
    "        \"max_model_len\": 1024,\n",
    "        \"enforce_eager\": False,  # Use CUDA graphs\n",
    "        \"trust_remote_code\": True,\n",
    "    }\n",
    "\n",
    "    # Add quantization if specified\n",
    "    if quantization:\n",
    "        config[\"quantization\"] = quantization\n",
    "        print(f\"\ud83d\udcca Using {quantization} quantization\")\n",
    "\n",
    "    # Add custom parameters\n",
    "    config.update(kwargs)\n",
    "\n",
    "    try:\n",
    "        llm = LLM(**config)\n",
    "        print(\"\u2705 vLLM model loaded successfully!\")\n",
    "        print(f\"[INFO] Model: {model_name}\")\n",
    "        # print(f\"\ud83e\udde0 Device: {llm.device}\") # Removed this line\n",
    "\n",
    "        # Attempt to clear GPU memory after loading the model\n",
    "        import gc\n",
    "        import torch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Attempting to clear GPU memory after model load...\")\n",
    "\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a small model\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "llm = setup_vllm_model(model_name, max_model_len=1024)\n",
    "\n",
    "# Display model information\n",
    "if llm:\n",
    "    print(f\"\\n\ud83d\udccb Model Configuration:\")\n",
    "    # print(f\"Max sequence length: {llm.max_model_len}\") # Removed this line\n",
    "    # print(f\"Vocabulary size: {llm.vocab_size}\") # Removed this line\n",
    "    # print(f\"Number of layers: {llm.num_layers}\") # Removed this line\n",
    "    # print(f\"Hidden size: {llm.hidden_size}\") # Removed this line\n",
    "    pass # Added pass to avoid empty if block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Basic Inference with vLLM\n",
    "\n",
    "Learn the fundamentals of running inference with vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inference example\n",
    "def basic_inference_demo(llm):\n",
    "    \"\"\"Demonstrate basic inference capabilities\"\"\"\n",
    "\n",
    "    if not llm:\n",
    "        print(\"\u274c No model loaded\")\n",
    "        return\n",
    "\n",
    "    # Sample prompts for classification\n",
    "    prompts = [\n",
    "        \"Classify this movie review: 'This film was absolutely amazing! The acting was superb and the plot kept me engaged throughout.'\",\n",
    "        \"Analyze the sentiment: 'I waited 2 hours in line and the product was out of stock. Terrible customer service.'\",\n",
    "        \"Determine if this is positive or negative: 'The food arrived cold and the portions were tiny for the price.'\",\n",
    "        \"Classify the emotion in: 'I just won the lottery! I can't believe my luck!'\"\n",
    "    ]\n",
    "\n",
    "    # Configure sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.1,  # Low temperature for consistent results\n",
    "        max_tokens=50,    # Limit response length\n",
    "        stop=[\"\\n\", \"\\r\"], # Stop at newlines\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0\n",
    "    )\n",
    "\n",
    "    print(\"\ud83d\udd0d Running basic inference...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n\u26a1 Inference completed in {total_time:.3f} seconds\")\n",
    "    print(f\"\ud83d\udcca Throughput: {len(prompts) / total_time:.2f} requests/second\")\n",
    "\n",
    "    # Display results\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f\"\\n\ud83d\udcdd Prompt {i+1}: {prompts[i][:60]}...\")\n",
    "        print(f\"\ud83e\udd16 Response: {output.outputs[0].text.strip()}\")\n",
    "        print(f\"\ud83d\udcca Tokens generated: {len(output.outputs[0].token_ids)}\")\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Run basic inference demo\n",
    "if llm:\n",
    "    results = basic_inference_demo(llm)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping inference demo - no model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Advanced Inference Techniques\n",
    "\n",
    "Exploring batched inference, continuous batching, and optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched inference performance comparison\n",
    "def benchmark_batch_sizes(llm, base_prompts: List[str], batch_sizes: List[int] = None):\n",
    "    \"\"\"Benchmark performance across different batch sizes\"\"\"\n",
    "\n",
    "    if not llm:\n",
    "        return None\n",
    "\n",
    "    if batch_sizes is None:\n",
    "        batch_sizes = [1, 2, 4, 8, 16]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n\ud83d\udd2c Testing batch size: {batch_size}\")\n",
    "\n",
    "        # Create prompts for this batch size\n",
    "        prompts = base_prompts * (batch_size // len(base_prompts) + 1)\n",
    "        prompts = prompts[:batch_size]\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            max_tokens=20,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "\n",
    "        # Time the inference\n",
    "        start_time = time.time()\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        throughput = len(prompts) / total_time\n",
    "        latency = total_time / len(prompts)\n",
    "\n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'total_time': total_time,\n",
    "            'throughput': throughput,\n",
    "            'latency': latency,\n",
    "            'total_tokens': sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "        })\n",
    "\n",
    "        print(f\"  \u23f1\ufe0f  Total time: {total_time:.3f}s\")\n",
    "        print(f\"  \u26a1 Throughput: {throughput:.2f} req/s\")\n",
    "        print(f\"  \ud83d\udd50 Latency: {latency:.3f}s per request\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run batch size benchmark\n",
    "base_prompts = [\n",
    "    \"Classify sentiment: This product is amazing!\",\n",
    "    \"Analyze: I love this service.\",\n",
    "    \"Determine polarity: Terrible experience.\"\n",
    "]\n",
    "\n",
    "if llm:\n",
    "    benchmark_results = benchmark_batch_sizes(llm, base_prompts, [1, 2, 4])\n",
    "\n",
    "    if benchmark_results is not None:\n",
    "        # Visualize results\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        ax1.plot(benchmark_results['batch_size'], benchmark_results['throughput'], 'bo-', linewidth=2)\n",
    "        ax1.set_xlabel('Batch Size')\n",
    "        ax1.set_ylabel('Throughput (req/s)')\n",
    "        ax1.set_title('Throughput vs Batch Size')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        ax2.plot(benchmark_results['batch_size'], benchmark_results['latency'], 'ro-', linewidth=2)\n",
    "        ax2.set_xlabel('Batch Size')\n",
    "        ax2.set_ylabel('Latency (s)')\n",
    "        ax2.set_title('Latency vs Batch Size')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n\ud83d\udcca Benchmark Summary:\")\n",
    "        print(benchmark_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Memory Management and Optimization\n",
    "\n",
    "Understanding memory usage and optimization techniques in vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring utilities\n",
    "def monitor_memory():\n",
    "    \"\"\"Monitor system and GPU memory usage\"\"\"\n",
    "\n",
    "    # CPU memory\n",
    "    cpu_memory = psutil.virtual_memory()\n",
    "    print(f\"\ud83d\udda5\ufe0f  CPU Memory: {cpu_memory.used / (1024**3):.2f}GB / {cpu_memory.total / (1024**3):.2f}GB ({cpu_memory.percent}%)\")\n",
    "\n",
    "    # GPU memory (if available)\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            gpu = gpus[0]\n",
    "            print(f\"\ud83c\udfae GPU Memory: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB ({gpu.memoryUtil * 100:.1f}%)\")\n",
    "            print(f\"\ud83c\udfae GPU Utilization: {gpu.load * 100:.1f}%\")\n",
    "    except:\n",
    "        print(\"\u26a0\ufe0f  GPU monitoring not available\")\n",
    "\n",
    "# Memory optimization techniques\n",
    "def memory_optimization_demo():\n",
    "    \"\"\"Demonstrate memory optimization techniques\"\"\"\n",
    "\n",
    "    print(\"\ud83e\udde0 Memory Optimization Techniques:\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    optimizations = {\n",
    "        \"PagedAttention\": \"Reduces memory fragmentation by virtualizing KV cache\",\n",
    "        \"Continuous Batching\": \"Dynamically batches requests for better utilization\",\n",
    "        \"Quantization\": \"Reduces model precision (FP16, INT8, INT4)\",\n",
    "        \"KV Cache Sharing\": \"Shares KV cache across similar sequences\",\n",
    "        \"Memory Pooling\": \"Pre-allocates memory to reduce allocation overhead\",\n",
    "        \"Gradient Checkpointing\": \"Trades compute for memory during inference\"\n",
    "    }\n",
    "\n",
    "    for technique, description in optimizations.items():\n",
    "        print(f\"\\n\ud83d\udd27 {technique}:\")\n",
    "        print(f\"   {description}\")\n",
    "\n",
    "    # Show current memory usage\n",
    "    print(\"\\n\ud83d\udcca Current Memory Status:\")\n",
    "    monitor_memory()\n",
    "\n",
    "memory_optimization_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Performance Profiling and Monitoring\n",
    "\n",
    "Learn how to monitor and profile vLLM performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance profiling utilities\n",
    "def profile_inference(llm, prompts: List[str], num_runs: int = 5):\n",
    "    \"\"\"Profile inference performance over multiple runs\"\"\"\n",
    "\n",
    "    if not llm:\n",
    "        return None\n",
    "\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=30,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\ud83d\udd2c Profiling {len(prompts)} prompts over {num_runs} runs...\")\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        throughput = len(prompts) / total_time\n",
    "        latency = total_time / len(prompts)\n",
    "\n",
    "        latencies.append(latency)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "        print(f\"Run {run+1}: {throughput:.2f} req/s, {latency:.3f}s latency\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    results = {\n",
    "        'mean_latency': np.mean(latencies),\n",
    "        'std_latency': np.std(latencies),\n",
    "        'mean_throughput': np.mean(throughputs),\n",
    "        'std_throughput': np.std(throughputs),\n",
    "        'min_latency': np.min(latencies),\n",
    "        'max_latency': np.max(latencies)\n",
    "    }\n",
    "\n",
    "    print(\"\\n\ud83d\udcca Profiling Results:\")\n",
    "    print(f\"Latency: {results['mean_latency']:.3f} \u00b1 {results['std_latency']:.3f}s\")\n",
    "    print(f\"Throughput: {results['mean_throughput']:.2f} \u00b1 {results['std_throughput']:.2f} req/s\")\n",
    "    print(f\"Latency Range: {results['min_latency']:.3f} - {results['max_latency']:.3f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Profile performance\n",
    "if llm:\n",
    "    profile_prompts = [\n",
    "        \"Analyze this review: Great product!\",\n",
    "        \"Classify sentiment: I love this.\",\n",
    "        \"Determine polarity: Not good.\"\n",
    "    ]\n",
    "\n",
    "    profile_results = profile_inference(llm, profile_prompts, num_runs=3)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping profiling - no model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Scaling and Distributed Inference\n",
    "\n",
    "Understanding how to scale vLLM for production workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling considerations\n",
    "def scaling_guide():\n",
    "    \"\"\"Guide for scaling vLLM deployments\"\"\"\n",
    "\n",
    "    scaling_strategies = {\n",
    "        \"Single GPU\": {\n",
    "            \"use_case\": \"Development, small applications\",\n",
    "            \"throughput\": \"100-500 req/s\",\n",
    "            \"setup\": \"tensor_parallel_size=1\"\n",
    "        },\n",
    "        \"Multi-GPU (Single Node)\": {\n",
    "            \"use_case\": \"Medium applications, A/B testing\",\n",
    "            \"throughput\": \"500-2000 req/s\",\n",
    "            \"setup\": \"tensor_parallel_size=2-4\"\n",
    "        },\n",
    "        \"Multi-Node Cluster\": {\n",
    "            \"use_case\": \"Large-scale production\",\n",
    "            \"throughput\": \"2000+ req/s\",\n",
    "            \"setup\": \"Ray-based distributed setup\"\n",
    "        },\n",
    "        \"Edge Deployment\": {\n",
    "            \"use_case\": \"Mobile, IoT applications\",\n",
    "            \"throughput\": \"10-100 req/s\",\n",
    "            \"setup\": \"Quantized models, CPU inference\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\ud83d\ude80 vLLM Scaling Strategies:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for strategy, details in scaling_strategies.items():\n",
    "        print(f\"\\n\ud83c\udfaf {strategy}:\")\n",
    "        print(f\"   \ud83d\udccb Use Case: {details['use_case']}\")\n",
    "        print(f\"   \u26a1 Throughput: {details['throughput']}\")\n",
    "        print(f\"   \ud83d\udd27 Setup: {details['setup']}\")\n",
    "\n",
    "    # Performance tips\n",
    "    print(\"\\n\ud83d\udca1 Performance Optimization Tips:\")\n",
    "    tips = [\n",
    "        \"Use continuous batching for dynamic workloads\",\n",
    "        \"Implement request prioritization for latency-sensitive tasks\",\n",
    "        \"Monitor GPU utilization and adjust batch sizes accordingly\",\n",
    "        \"Use model parallelism for very large models\",\n",
    "        \"Implement caching for frequent requests\",\n",
    "        \"Consider CPU offloading for memory-intensive tasks\"\n",
    "    ]\n",
    "\n",
    "    for i, tip in enumerate(tips, 1):\n",
    "        print(f\"   {i}. {tip}\")\n",
    "\n",
    "scaling_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Key Takeaways\n",
    "\n",
    "1. **vLLM Architecture**: Continuous batching, PagedAttention, and optimized memory management\n",
    "2. **Performance Optimization**: Batch size tuning, quantization, and hardware utilization\n",
    "3. **Production Readiness**: Monitoring, error handling, and scaling strategies\n",
    "4. **Memory Management**: KV cache optimization and memory pooling\n",
    "5. **Quantization**: Trade-offs between speed, memory, and accuracy\n",
    "6. **Monitoring**: Throughput, latency, and resource utilization tracking\n",
    "\n",
    "## \ud83d\ude80 Next Steps\n",
    "\n",
    "Now that you understand efficient inference, proceed to:\n",
    "- **Notebook 3**: Advanced Fine-tuning with Unsloth and PEFT\n",
    "- **Notebook 4**: Production Deployment and Scaling\n",
    "- **Notebook 5**: Evaluation, Benchmarking, and Ethics\n",
    "\n",
    "## \ud83d\udd17 Additional Resources\n",
    "\n",
    "- [vLLM Documentation](https://vllm.readthedocs.io/)\n",
    "- [Efficient Inference Techniques](https://arxiv.org/abs/2205.05198)\n",
    "- [PagedAttention Paper](https://arxiv.org/abs/2309.06180)\n",
    "- [Model Compression Techniques](https://arxiv.org/abs/2002.11794)\n",
    "\n",
    "## \ud83c\udfaf Hands-on Exercises\n",
    "\n",
    "1. **Batch Size Optimization**: Experiment with different batch sizes and measure throughput/latency trade-offs\n",
    "2. **Quantization Comparison**: Compare FP16, INT8, and INT4 quantization on the same model\n",
    "3. **Memory Profiling**: Monitor GPU memory usage during inference with different configurations\n",
    "4. **Production Service**: Build a simple REST API using the ProductionLLMService class\n",
    "5. **Performance Benchmarking**: Create a comprehensive benchmark comparing different inference engines\n",
    "\n",
    "## \ud83c\udf89 Conclusion\n",
    "\n",
    "You've now mastered efficient LLM inference with vLLM! Key achievements:\n",
    "- \u2705 Understanding vLLM's core optimizations\n",
    "- \u2705 Implementing batched and continuous batching\n",
    "- \u2705 Memory management and optimization techniques\n",
    "- \u2705 Quantization strategies and trade-offs\n",
    "- \u2705 Production-ready service implementation\n",
    "- \u2705 Performance profiling and monitoring\n",
    "- \u2705 Scaling strategies for different workloads\n",
    "\n",
    "Ready to move on to advanced fine-tuning techniques! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}