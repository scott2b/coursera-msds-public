{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/MSDSTextClassification_Lab2_KTrain.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Classification, Lab 2: Building a model with K-Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About this lab"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first part of this lab is identical to Lab 1, and you should be able to step through it easily.\n",
    "\n",
    "We continue now with the goal of building an inference model for predicting whether or not a document is about \"healthy living.\" We will use the K-Train library to do this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About the final project"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Recall that you are working toward a final project. After completing this lab, you will want to go the extra mile and explore ways to tweak and improve your model. See the final project description for further details on what is expected."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "The data file for this lab is available in the Coursera lab environment in the `data/` directory. To use it in Colab:\n",
    "\n",
    "1. In the Coursera lab, click the **Files** tab and navigate to `lab2 / data`\n",
    "2. Select the checkbox next to `news_category_trainingdata.json`, then click the **Download** button in the toolbar\n",
    "3. In your Google Drive, create a folder called `coursera-msds` (if you haven't already)\n",
    "4. Upload the downloaded file to `coursera-msds/`\n",
    "\n",
    "The code below will mount your Google Drive and load the data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going to be using Google's Tensorflow package:\n",
    "https://www.tensorflow.org/tutorials\n",
    "\n",
    "We're using an API wrapper for Tensorflow called ktrain. It's absolutely fabulous because it really abstracts the whole deep learning process into a workflow so easy, even a computational social scientist can do it:\n",
    "https://github.com/amaiya/ktrain"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "try:\n",
    "    from transformers import TFAutoModel  # removed in transformers v5\n",
    "    import ktrain\n",
    "    from ktrain import text\n",
    "except ImportError:\n",
    "    # ktrain needs TF model classes from transformers v4.x\n",
    "    !pip uninstall -y transformers\n",
    "    !pip install \"transformers>=4.43,<5\" \"numpy<2\" ktrain tf-keras\n",
    "    os.kill(os.getpid(), 9)  # restart runtime; cell re-runs cleanly\n",
    "\n",
    "import ktrain\n",
    "import numpy as np\n",
    "np.Inf = np.inf  # compatibility shim: np.Inf removed in NumPy 2.0\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_FILENAME = 'news_category_trainingdata.json'\n",
    "DRIVE_PATH = f'/content/drive/MyDrive/coursera-msds/{DATA_FILENAME}'\n",
    "LOCAL_PATH = f'data/{DATA_FILENAME}'\n",
    "\n",
    "if os.path.exists(DRIVE_PATH):\n",
    "    data_path = DRIVE_PATH\n",
    "    print(f'Loading data from Google Drive: {DRIVE_PATH}')\n",
    "elif os.path.exists(LOCAL_PATH):\n",
    "    data_path = LOCAL_PATH\n",
    "    print(f'Loading data from local path: {LOCAL_PATH}')\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f'Data file not found. Please either:\\n'\n",
    "        f'  1. Upload {DATA_FILENAME} to Google Drive at: MyDrive/coursera-msds/\\n'\n",
    "        f'  2. Place {DATA_FILENAME} in a local data/ directory'\n",
    "    )\n",
    "\n",
    "reviews = pd.read_json(data_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspect the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "reviews.head()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most machine learning tools in Python accept one field/column/string. So we have to merge our two text column. Let's separate it with a space."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "reviews['combined_text'] = reviews['headline'] + ' ' + reviews['short_description']"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing we need to do is prepare the data. Specifically, we have a categorical column that we want to turn into a \"is this article healthy living?\" column. That is, when an article is about healthy living, it should have a 1, when it's anything else, it should be a 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "reviews[reviews['category'].str.contains(\"HEALTHY LIVING\")]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reviews['healthy'] = np.where((reviews['category'] == 'HEALTHY LIVING'), 1, 0)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reviews['healthy'].describe()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Balance the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our data is very unbalanced \u2014 there are far more non-health articles than health articles. We balance the dataset by sampling equal instances of both classes.\n",
    "\n",
    "Set `SAMPLE_PER_CLASS` to `None` to use the full balanced dataset (slower but potentially higher accuracy). The default of 1000 keeps training quick on a T4 GPU. After completing the lab, consider increasing the sample size to see if you can improve model performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Balance the dataset by sampling equal instances of both classes.\n",
    "# Set SAMPLE_PER_CLASS to None to use the full dataset (slower but\n",
    "# potentially higher accuracy).\n",
    "SAMPLE_PER_CLASS = 1000  # set to None for full dataset\n",
    "\n",
    "healthy = reviews[reviews['healthy'] == 1]\n",
    "not_healthy = reviews[reviews['healthy'] == 0]\n",
    "\n",
    "if SAMPLE_PER_CLASS is not None:\n",
    "    healthy = healthy.sample(n=min(SAMPLE_PER_CLASS, len(healthy)), random_state=42)\n",
    "    not_healthy = not_healthy.sample(n=min(SAMPLE_PER_CLASS, len(not_healthy)), random_state=42)\n",
    "\n",
    "sample_amount = min(len(healthy), len(not_healthy))\n",
    "healthy = healthy.head(sample_amount)\n",
    "not_healthy = not_healthy.head(sample_amount)\n",
    "review_sample = pd.concat([healthy, not_healthy])\n",
    "print(f'Training with {len(review_sample)} samples ({sample_amount} per class)')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "review_sample.describe()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On to Lab 2: Test, Tune and Save Models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, you will tune and train a predictor model for classifying healthy-living articles. After completing this lab, complete the Lab Quiz by entering your precision and recall values from the validation report for both the negative and positive classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "target_names = ['NOT HEALTHY LIVING','HEALTHY LIVING']"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Experimenting with different transformers\n",
    "\n",
    "For purposes of this lab, we are using the **distilbert-base-uncased** transformer model. Other models you might try for your final project include:\n",
    "\n",
    " * roberta-base\n",
    " * bert-base-uncased\n",
    " * distilroberta-base\n",
    "\n",
    "See all the models here: https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "Some work, some dont, try at your own risk.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train, val, preprocess = ktrain.text.texts_from_df(\n",
    "    review_sample,\n",
    "    \"combined_text\",\n",
    "    label_columns=[\"healthy\"],\n",
    "    val_df=None,\n",
    "    max_features=20000,\n",
    "    maxlen=512,\n",
    "    val_pct=0.1,\n",
    "    ngram_range=1,\n",
    "    preprocess_mode=\"distilbert\",\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = preprocess.get_classifier()\n",
    "learner = ktrain.get_learner(model, train_data=train, val_data=val, batch_size=16)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "learner.lr_find(max_epochs=6)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "learner.lr_plot()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, use the tuned learner to train the best model.\n",
    "\n",
    "Here, we define a limit of 10 epochs, but in reality, this should stop much sooner due to early stopping."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "history=learner.autofit(\n",
    "    1e-4,\n",
    "    checkpoint_folder='checkpoint',\n",
    "    epochs=10,\n",
    "    early_stopping=True\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the predictor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc=preprocess)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optionally, uncomment this code to save the predictor and reload it later. Note, the saved models can be quite large and may quickly use up space on your Google Drive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#predictor.save(\"drive/MyDrive/MSDSTextClassification_Lab2.healthy_living\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "validation = learner.validate(val_data=val, print_report=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## \ud83e\uddd0 Lab Quiz Questions 1-4\n",
    "\n",
    "Enter the following values from the validation report into the Lab Quiz:\n",
    "\n",
    " 1. precision for non healthy-living articles\n",
    " 2. recall for non healthy-living articles\n",
    " 3. precision for healthy-living articles\n",
    " 4. recall for healthy-living articles\n",
    "\n",
    " ---\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep in mind that we've reduced the training set for the sake of expediency. For your final analysis and project, you should complete a run of the full data set. Pay attention to the impact of the input data on the performance of the final model (i.e. the validation scores)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspecting the drivers of prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No matter what the supervised machine learning model, you always want to peak under the hood to see what features are driving prediction. That is, what words sway the outcome of the prediction. It's harder to inspect a neural network. Because all of the layers of a neural network aren't really interpretable to the human eye.\n",
    "\n",
    "Currently, the best practice I've found is a little tool Explainable AI:\n",
    "https://alvinntnu.github.io/python-notes/nlp/ktrain-tutorial-explaining-predictions.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip3 install -q git+https://github.com/amaiya/eli5@tfkeras_0_10_1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's go ahead and make a little set of test documents to check out"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_docs = [\n",
    "'Stress May Be Your Heart\u2019s Worst Enemy Psychological stress activates the fear center in the brain, setting into motion a cascade of reactions that can lead to heart attacks and strokes.',\n",
    "'Exercising to Slim Down? Try Getting Bigger. It\u2019s high time for women to reclaim the real strength behind exercise.',\n",
    "'What Are Your Food Resolutions for the New Year? Join us for the Eat Well Challenge starting in January.',\n",
    "'Why We All Need to Have More Fun. Prioritizing fun may feel impossible right now. But this four-step plan will help you rediscover how to feel more alive.',\n",
    "'Cuomo Will Not Be Prosecuted in Groping Case, Albany D.A. Says. The district attorney described the woman who said former Gov. Andrew Cuomo had groped her as \u201ccredible,\u201d but added that proving her allegation would be difficult.',\n",
    "'A Film Captures Jewish Life in a Polish Town Before the Nazis Arrived. A documentary based on a home movie shot by an American in 1938 provides a look at the vibrancy of a Jewish community in Europe just before the Holocaust.'\n",
    "             ]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i, text in enumerate(test_docs):\n",
    "  probs = predictor.predict(text, return_proba=True)\n",
    "  print(\"---------------------------\")\n",
    "  print('The probability this is healthy is %s' % probs[1])\n",
    "  print(text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*These* are pretty obvious examples, but it works exactly as expected!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "predictor.explain('Diversity is the key to a healthy society. Here is what we need to do to make america a more equitable place to live for all.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "But you can see, this algorithm is far from perfect. Here you can see that it's probably got too high of an emphasis on the word \"healthy.\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So what would I do next? Well, given that it's over reacting to worrds like health and equitable, I'd try introducing more negative examples into the data, times where healthy is used outside of health and wellness news. We can do this by changing our sample from 50/50 to something like 20/80, but of course, the more documents we process, the longer this model is going to take to make!"
   ],
   "metadata": {}
  }
 ]
}