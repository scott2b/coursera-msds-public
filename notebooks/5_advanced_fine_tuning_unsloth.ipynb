{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/5_advanced_fine_tuning_unsloth.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Advanced Fine-tuning with Unsloth and PEFT\n",
    "\n",
    "This notebook provides comprehensive coverage of modern fine-tuning techniques using Unsloth and Parameter-Efficient Fine-Tuning (PEFT) methods.\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Master Parameter-Efficient Fine-Tuning (PEFT) techniques\n",
    "2. Understand and implement LoRA, QLoRA, and other efficient methods\n",
    "3. Use Unsloth for ultra-fast fine-tuning\n",
    "4. Implement advanced training strategies and optimizations\n",
    "5. Handle memory constraints with gradient checkpointing\n",
    "6. Fine-tune for classification, generation, and instruction following\n",
    "7. Evaluate and compare different fine-tuning approaches\n",
    "8. Deploy fine-tuned models for production use\n",
    "\n",
    "## \ud83d\udd27 Prerequisites\n",
    "\n",
    "- Completed Notebooks 1 & 2 (LLM Fundamentals and vLLM Inference)\n",
    "- Understanding of transformers and attention mechanisms\n",
    "- Basic knowledge of PyTorch and training loops\n",
    "- Familiarity with classification tasks and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages not pre-installed in Colab\n",
    "!pip install transformers datasets accelerate peft trl\n",
    "!pip install unsloth bitsandbytes\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig, pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, get_peft_model, prepare_model_for_kbit_training,\n",
    "    TaskType, PeftModel, PeftConfig\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\ud83c\udfaf Advanced Fine-tuning Environment Ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfa8 Understanding PEFT Methods\n",
    "\n",
    "Learn about Parameter-Efficient Fine-Tuning techniques and their advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT method comparison\n",
    "peft_methods = {\n",
    "    \"LoRA (Low-Rank Adaptation)\": {\n",
    "        \"description\": \"Learns low-rank updates to weight matrices\",\n",
    "        \"parameters\": \"~0.5-2% of original\",\n",
    "        \"memory\": \"Minimal increase\",\n",
    "        \"training_speed\": \"~1-2x slower than full fine-tuning\",\n",
    "        \"inference\": \"Negligible overhead\",\n",
    "        \"best_for\": \"Fine-tuning large models, memory-constrained environments\"\n",
    "    },\n",
    "    \"QLoRA (Quantized LoRA)\": {\n",
    "        \"description\": \"Combines quantization with LoRA for extreme efficiency\",\n",
    "        \"parameters\": \"~0.2-1% of original\",\n",
    "        \"memory\": \"~70% reduction vs full fine-tuning\",\n",
    "        \"training_speed\": \"~2-3x faster than LoRA\",\n",
    "        \"inference\": \"Requires quantization-aware serving\",\n",
    "        \"best_for\": \"Very large models, extreme memory constraints\"\n",
    "    },\n",
    "    \"AdaLoRA\": {\n",
    "        \"description\": \"Dynamically allocates parameters based on importance\",\n",
    "        \"parameters\": \"Adaptive (0.5-5%)\",\n",
    "        \"memory\": \"Adaptive reduction\",\n",
    "        \"training_speed\": \"Similar to LoRA\",\n",
    "        \"inference\": \"Minimal overhead\",\n",
    "        \"best_for\": \"Dynamic parameter allocation, task-specific optimization\"\n",
    "    },\n",
    "    \"Prompt Tuning\": {\n",
    "        \"description\": \"Learns soft prompts instead of model weights\",\n",
    "        \"parameters\": \"~0.01% of original\",\n",
    "        \"memory\": \"Negligible\",\n",
    "        \"training_speed\": \"Very fast\",\n",
    "        \"inference\": \"Minimal overhead\",\n",
    "        \"best_for\": \"Few-shot learning, rapid adaptation\"\n",
    "    },\n",
    "    \"P-Tuning\": {\n",
    "        \"description\": \"Learns prompt embeddings with continuous optimization\",\n",
    "        \"parameters\": \"~0.1% of original\",\n",
    "        \"memory\": \"Very low\",\n",
    "        \"training_speed\": \"Fast\",\n",
    "        \"inference\": \"Minimal overhead\",\n",
    "        \"best_for\": \"Natural language understanding tasks\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display comparison table\n",
    "comparison_data = []\n",
    "for method, details in peft_methods.items():\n",
    "    comparison_data.append({\n",
    "        'Method': method,\n",
    "        'Parameters': details['parameters'],\n",
    "        'Memory': details['memory'],\n",
    "        'Training Speed': details['training_speed'],\n",
    "        'Best For': details['best_for']\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\ud83d\udd2c PEFT Methods Comparison:\")\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Unsloth Setup and Configuration\n",
    "\n",
    "Learn how to set up and configure Unsloth for ultra-fast fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth setup\n",
    "def setup_unsloth_model(model_name: str, max_seq_length: int = 2048):\n",
    "    \"\"\"Setup Unsloth model for efficient fine-tuning\"\"\"\n",
    "\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "\n",
    "        print(f\"\ud83d\ude80 Setting up Unsloth with {model_name}...\")\n",
    "\n",
    "        # Load model with Unsloth optimizations\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=None,  # Auto-detect\n",
    "            load_in_4bit=True,  # Use 4-bit quantization\n",
    "        )\n",
    "\n",
    "        print(\"\u2705 Unsloth model loaded successfully!\")\n",
    "        print(f\"\ud83d\udcca Model: {model_name}\")\n",
    "        print(f\"\ud83c\udfaf Max sequence length: {max_seq_length}\")\n",
    "        print(f\"\ud83e\udde0 Quantization: 4-bit\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\u274c Unsloth not available. Falling back to standard PEFT...\")\n",
    "\n",
    "        # Fallback to standard transformers + PEFT\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Prepare for PEFT\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "# Setup model\n",
    "model_name = \"unsloth/mistral-7b-bnb-4bit\"  # Try Unsloth first\n",
    "try:\n",
    "    model, tokenizer = setup_unsloth_model(model_name)\n",
    "    using_unsloth = True\n",
    "except:\n",
    "    # Fallback model\n",
    "    model_name = \"microsoft/DialoGPT-medium\"\n",
    "    model, tokenizer = setup_unsloth_model(model_name)\n",
    "    using_unsloth = False\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Using {'Unsloth' if using_unsloth else 'Standard PEFT'}\")\n",
    "print(f\"\ud83d\udcca Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"\ud83c\udfaf Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "def setup_lora_config(model, r: int = 16, alpha: int = 16, dropout: float = 0.0):\n",
    "    \"\"\"Setup LoRA configuration for the model\"\"\"\n",
    "\n",
    "    if using_unsloth:\n",
    "        from unsloth import FastLanguageModel\n",
    "\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=r,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                           \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=dropout,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=3407,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None,\n",
    "        )\n",
    "    else:\n",
    "        # Standard PEFT LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=alpha,\n",
    "            target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 style\n",
    "            lora_dropout=dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Print trainable parameters info\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(\"\ud83c\udfaf LoRA Configuration:\")\n",
    "    print(f\"   Rank (r): {r}\")\n",
    "    print(f\"   Alpha: {alpha}\")\n",
    "    print(f\"   Dropout: {dropout}\")\n",
    "    print(f\"   Trainable params: {trainable_params:,}\")\n",
    "    print(f\"   Total params: {total_params:,}\")\n",
    "    print(f\"   Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Setup LoRA\n",
    "model = setup_lora_config(model, r=16, alpha=16, dropout=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Data Preparation for Fine-tuning\n",
    "\n",
    "Learn how to prepare and format data for effective fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "def load_classification_dataset():\n",
    "    \"\"\"Load and prepare a classification dataset for fine-tuning\"\"\"\n",
    "\n",
    "    # Load IMDB dataset\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:10%]\")  # Small subset for demo\n",
    "\n",
    "    print(f\"\ud83d\udcca Dataset loaded: {len(dataset)} examples\")\n",
    "    print(f\"\ud83d\udcdd Sample: {dataset[0]}\")\n",
    "\n",
    "    # Convert to instruction format\n",
    "    def format_instruction(example):\n",
    "        instruction = \"Classify the sentiment of this movie review as positive or negative.\"\n",
    "\n",
    "        formatted_text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{example['text']}\n",
    "\n",
    "### Response:\n",
    "{'positive' if example['label'] == 1 else 'negative'}\n",
    "\"\"\"\n",
    "\n",
    "        return {\"text\": formatted_text, \"label\": example[\"label\"]}\n",
    "\n",
    "    # Format dataset\n",
    "    formatted_dataset = dataset.map(format_instruction)\n",
    "\n",
    "    print(\"\\n\ud83d\udd04 Dataset formatted for instruction tuning\")\n",
    "    print(f\"\ud83d\udcdd Formatted sample: {formatted_dataset[0]['text'][:200]}...\")\n",
    "\n",
    "    return formatted_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_classification_dataset()\n",
    "\n",
    "# Split dataset\n",
    "train_val_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Training set: {len(train_dataset)} examples\")\n",
    "print(f\"\ud83d\udcca Validation set: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Fine-tuning Implementation\n",
    "\n",
    "Learn how to implement and execute fine-tuning with modern techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # Short training for demo\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=50,  # Very short for demo\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    eval_steps=25,\n",
    "    eval_strategy=\"steps\", # Set eval_strategy to match save_strategy\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # Disable wandb for demo\n",
    ")\n",
    "\n",
    "# Custom data collator for instruction tuning\n",
    "def data_collator(features):\n",
    "    \"\"\"Custom data collator for instruction tuning\"\"\"\n",
    "    batch = tokenizer(\n",
    "        [f[\"text\"] for f in features],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return batch\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\ud83d\ude80 Starting fine-tuning...\")\n",
    "print(f\"\ud83d\udcca Training for {training_args.max_steps} steps\")\n",
    "print(f\"\ud83d\udcca Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"\ud83d\udcca Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"\ud83d\udcca Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "# Note: Actual training commented out for demo\n",
    "# trainer.train()\n",
    "print(\"\\n\u2705 Fine-tuning setup complete! (Training commented out for demo)\")\n",
    "print(\"\ud83d\udca1 To run actual training, uncomment trainer.train() above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Training Monitoring and Evaluation\n",
    "\n",
    "Learn how to monitor training progress and evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics and evaluation\n",
    "def evaluate_fine_tuned_model(model, tokenizer, test_dataset):\n",
    "    \"\"\"Evaluate the fine-tuned model on test data\"\"\"\n",
    "\n",
    "    print(\"\ud83d\udd0d Evaluating fine-tuned model...\")\n",
    "\n",
    "    # Load evaluation metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate on a small subset\n",
    "    eval_subset = test_dataset.select(range(min(20, len(test_dataset))))\n",
    "\n",
    "    for example in eval_subset:\n",
    "        # Format prompt\n",
    "        prompt = f\"\"\"Classify the sentiment of this movie review as positive or negative.\n",
    "\n",
    "Review: {example['text']}\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "        # Generate prediction\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode prediction\n",
    "        generated_text = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "        # Extract sentiment (simple keyword matching)\n",
    "        pred_sentiment = 1 if \"positive\" in generated_text.lower() else 0\n",
    "        true_sentiment = example['label']\n",
    "\n",
    "        predictions.append(pred_sentiment)\n",
    "        references.append(true_sentiment)\n",
    "\n",
    "        print(f\"\ud83d\udcdd True: {'positive' if true_sentiment else 'negative'}, Pred: {'positive' if pred_sentiment else 'negative'}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(\"\\n\ud83d\udcca Evaluation Results:\")\n",
    "    print(f\"   Accuracy: {accuracy['accuracy']:.3f}\")\n",
    "    print(f\"   F1-Score: {f1['f1']:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy['accuracy'],\n",
    "        \"f1\": f1['f1'],\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "# Note: Evaluation commented out since training was skipped\n",
    "print(\"\ud83d\udcca Evaluation setup complete (would run after training)\")\n",
    "# eval_results = evaluate_fine_tuned_model(model, tokenizer, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Advanced Training Techniques\n",
    "\n",
    "Explore advanced training strategies and optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced training techniques\n",
    "def demonstrate_advanced_training():\n",
    "    \"\"\"Demonstrate advanced training techniques\"\"\"\n",
    "\n",
    "    techniques = {\n",
    "        \"Gradient Checkpointing\": {\n",
    "            \"purpose\": \"Reduce memory usage during training\",\n",
    "            \"trade_off\": \"~20% slower training, ~50% less memory\",\n",
    "            \"when_to_use\": \"Large models, limited GPU memory\",\n",
    "            \"implementation\": \"model.gradient_checkpointing_enable()\"\n",
    "        },\n",
    "        \"Mixed Precision Training\": {\n",
    "            \"purpose\": \"Faster training with FP16\",\n",
    "            \"trade_off\": \"Potential numerical instability\",\n",
    "            \"when_to_use\": \"Modern GPUs with Tensor Cores\",\n",
    "            \"implementation\": \"TrainingArguments(fp16=True)\"\n",
    "        },\n",
    "        \"Gradient Accumulation\": {\n",
    "            \"purpose\": \"Simulate larger batch sizes\",\n",
    "            \"trade_off\": \"Slower training per step\",\n",
    "            \"when_to_use\": \"Limited GPU memory, want stable training\",\n",
    "            \"implementation\": \"gradient_accumulation_steps=N\"\n",
    "        },\n",
    "        \"Learning Rate Scheduling\": {\n",
    "            \"purpose\": \"Dynamic learning rate adjustment\",\n",
    "            \"trade_off\": \"Requires hyperparameter tuning\",\n",
    "            \"when_to_use\": \"Long training runs, stable convergence\",\n",
    "            \"implementation\": \"lr_scheduler_type='cosine'\"\n",
    "        },\n",
    "    \"Early Stopping\": {\n",
    "            \"purpose\": \"Prevent overfitting\",\n",
    "            \"trade_off\": \"May stop training too early\",\n",
    "            \"when_to_use\": \"Limited compute, validation data available\",\n",
    "            \"implementation\": \"EarlyStoppingCallback()\"\n",
    "        },\n",
    "        \"Weight Decay\": {\n",
    "            \"purpose\": \"Regularization to prevent overfitting\",\n",
    "            \"trade_off\": \"May slow convergence\",\n",
    "            \"when_to_use\": \"Small datasets, prone to overfitting\",\n",
    "            \"implementation\": \"weight_decay=0.01\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\ud83d\udee0\ufe0f  Advanced Training Techniques:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for technique, details in techniques.items():\n",
    "        print(f\"\\n\ud83c\udfaf {technique}:\")\n",
    "        print(f\"   \ud83d\udccb Purpose: {details['purpose']}\")\n",
    "        print(f\"   \u2696\ufe0f  Trade-off: {details['trade_off']}\")\n",
    "        print(f\"   \ud83c\udfaf When to use: {details['when_to_use']}\")\n",
    "        print(f\"   \ud83d\udcbb Implementation: {details['implementation']}\")\n",
    "\n",
    "demonstrate_advanced_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Model Saving and Merging\n",
    "\n",
    "Learn how to save, merge, and deploy fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving and merging\n",
    "def save_and_merge_model(model, tokenizer, output_dir: str = \"./fine_tuned_model\"):\n",
    "    \"\"\"Save and optionally merge the fine-tuned model\"\"\"\n",
    "\n",
    "    print(f\"\ud83d\udcbe Saving model to {output_dir}...\")\n",
    "\n",
    "    # Save the PEFT model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    print(\"\u2705 PEFT adapters saved!\")\n",
    "\n",
    "    # Optionally merge with base model\n",
    "    try:\n",
    "        print(\"\ud83d\udd04 Merging LoRA weights with base model...\")\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Merge\n",
    "        merged_model = model.merge_and_unload()\n",
    "\n",
    "        # Save merged model\n",
    "        merged_model.save_pretrained(f\"{output_dir}_merged\")\n",
    "        tokenizer.save_pretrained(f\"{output_dir}_merged\")\n",
    "\n",
    "        print(\"\u2705 Merged model saved!\")\n",
    "\n",
    "        return merged_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Could not merge model: {e}\")\n",
    "        print(\"\ud83d\udca1 PEFT model still usable for inference\")\n",
    "        return model\n",
    "\n",
    "# Save model\n",
    "merged_model = save_and_merge_model(model, tokenizer, \"./fine_tuned_sentiment_model\")\n",
    "print(\"\\n\ud83c\udf89 Model saving complete!\")\n",
    "print(\"\ud83d\udcc1 PEFT model: ./fine_tuned_sentiment_model\")\n",
    "print(\"\ud83d\udcc1 Merged model: ./fine_tuned_sentiment_model_merged\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Comparing Fine-tuning Methods\n",
    "\n",
    "Compare different fine-tuning approaches and their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning method comparison\n",
    "def compare_fine_tuning_methods():\n",
    "    \"\"\"Compare different fine-tuning approaches\"\"\"\n",
    "\n",
    "    methods = {\n",
    "        \"Full Fine-tuning\": {\n",
    "            \"parameters\": \"100% of model\",\n",
    "            \"memory\": \"High (full model)\",\n",
    "            \"training_time\": \"Long\",\n",
    "            \"performance\": \"Best (potentially)\",\n",
    "            \"flexibility\": \"High\",\n",
    "            \"use_case\": \"Small models, unlimited compute\"\n",
    "        },\n",
    "        \"LoRA\": {\n",
    "            \"parameters\": \"0.5-2% of model\",\n",
    "            \"memory\": \"Low\",\n",
    "            \"training_time\": \"Medium\",\n",
    "            \"performance\": \"Very good\",\n",
    "            \"flexibility\": \"High\",\n",
    "            \"use_case\": \"Large models, limited compute\"\n",
    "        },\n",
    "        \"QLoRA\": {\n",
    "            \"parameters\": \"0.2-1% of model\",\n",
    "            \"memory\": \"Very low\",\n",
    "            \"training_time\": \"Fast\",\n",
    "            \"performance\": \"Good\",\n",
    "            \"flexibility\": \"Medium\",\n",
    "            \"use_case\": \"Very large models, extreme constraints\"\n",
    "        },\n",
    "        \"Prompt Tuning\": {\n",
    "            \"parameters\": \"<0.01% of model\",\n",
    "            \"memory\": \"Minimal\",\n",
    "            \"training_time\": \"Very fast\",\n",
    "            \"performance\": \"Fair\",\n",
    "            \"flexibility\": \"Low\",\n",
    "            \"use_case\": \"Few-shot learning, rapid prototyping\"\n",
    "        },\n",
    "        \"Instruction Tuning\": {\n",
    "            \"parameters\": \"Varies (0.1-5%)\",\n",
    "            \"memory\": \"Low to medium\",\n",
    "            \"training_time\": \"Medium\",\n",
    "            \"performance\": \"Excellent\",\n",
    "            \"flexibility\": \"High\",\n",
    "            \"use_case\": \"Chatbots, instruction-following tasks\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    df = pd.DataFrame.from_dict(methods, orient='index')\n",
    "    df.index.name = 'Method'\n",
    "\n",
    "    print(\"\ud83d\udd2c Fine-tuning Methods Comparison:\")\n",
    "    print(df.to_string())\n",
    "\n",
    "    # Performance vs Efficiency plot\n",
    "    methods_short = ['Full FT', 'LoRA', 'QLoRA', 'Prompt', 'Instruction']\n",
    "    performance_scores = [9, 8, 7, 5, 8.5]  # Subjective performance scores\n",
    "    efficiency_scores = [1, 7, 9, 10, 8]    # Efficiency scores (higher = more efficient)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(efficiency_scores, performance_scores, s=100)\n",
    "\n",
    "    for i, method in enumerate(methods_short):\n",
    "        plt.annotate(method, (efficiency_scores[i], performance_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "    plt.xlabel('Efficiency Score')\n",
    "    plt.ylabel('Performance Score')\n",
    "    plt.title('Fine-tuning Methods: Performance vs Efficiency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "comparison_df = compare_fine_tuning_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Production Deployment Considerations\n",
    "\n",
    "Learn how to deploy fine-tuned models for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment considerations\n",
    "def production_deployment_guide():\n",
    "    \"\"\"Guide for deploying fine-tuned models in production\"\"\"\n",
    "\n",
    "    considerations = {\n",
    "        \"Model Format\": {\n",
    "            \"PEFT\": \"Small, fast loading, requires base model\",\n",
    "            \"Merged\": \"Larger, self-contained, slower loading\",\n",
    "            \"Quantized\": \"Smallest, fastest inference, accuracy trade-off\",\n",
    "            \"Recommendation\": \"PEFT for development, Merged for production\"\n",
    "        },\n",
    "        \"Inference Optimization\": {\n",
    "            \"Batch Processing\": \"Use vLLM or similar for batched inference\",\n",
    "            \"Caching\": \"Cache frequent prompts and responses\",\n",
    "            \"Model Parallelism\": \"Distribute large models across GPUs\",\n",
    "            \"Quantization\": \"Use 4-bit or 8-bit for faster inference\"\n",
    "        },\n",
    "        \"Monitoring\": {\n",
    "            \"Performance Metrics\": \"Latency, throughput, memory usage\",\n",
    "            \"Model Metrics\": \"Accuracy, F1-score, drift detection\",\n",
    "            \"System Metrics\": \"CPU/GPU utilization, error rates\",\n",
    "            \"Business Metrics\": \"User satisfaction, task completion\"\n",
    "        },\n",
    "        \"Scaling\": {\n",
    "            \"Horizontal Scaling\": \"Multiple model instances\",\n",
    "            \"Vertical Scaling\": \"Larger GPUs, more memory\",\n",
    "            \"Load Balancing\": \"Distribute requests across instances\",\n",
    "            \"Auto-scaling\": \"Scale based on demand patterns\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\ud83d\ude80 Production Deployment Guide:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, details in considerations.items():\n",
    "        print(f\"\\n\ud83d\udccb {category}:\")\n",
    "        for key, value in details.items():\n",
    "            print(f\"   \u2022 {key}: {value}\")\n",
    "\n",
    "production_deployment_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Key Takeaways\n",
    "\n",
    "1. **PEFT Methods**: LoRA, QLoRA, and other parameter-efficient techniques dramatically reduce training costs\n",
    "2. **Unsloth**: Ultra-fast fine-tuning with automatic optimizations\n",
    "3. **Memory Management**: Gradient checkpointing and quantization enable training of large models\n",
    "4. **Data Formatting**: Proper instruction tuning format is crucial for good performance\n",
    "5. **Evaluation**: Comprehensive metrics beyond just loss (accuracy, F1, perplexity)\n",
    "6. **Model Merging**: Combining LoRA adapters with base models for deployment\n",
    "7. **Production Ready**: Monitoring, scaling, and optimization considerations\n",
    "\n",
    "## \ud83d\ude80 Next Steps\n",
    "\n",
    "Now that you understand advanced fine-tuning, proceed to:\n",
    "- **Notebook 4**: Production Deployment and Scaling\n",
    "- **Notebook 5**: Evaluation, Benchmarking, and Ethics\n",
    "\n",
    "## \ud83d\udd17 Additional Resources\n",
    "\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685) - Original LoRA research\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314) - Quantized LoRA\n",
    "- [Unsloth Documentation](https://github.com/unslothai/unsloth)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/index)\n",
    "\n",
    "## \ud83c\udfaf Hands-on Exercises\n",
    "\n",
    "1. **LoRA Configuration**: Experiment with different LoRA ranks (8, 16, 32) and alpha values\n",
    "2. **Quantization Comparison**: Compare FP16, 8-bit, and 4-bit training performance\n",
    "3. **Dataset Formatting**: Try different instruction formats and compare results\n",
    "4. **Hyperparameter Tuning**: Use Optuna to optimize learning rate and batch size\n",
    "5. **Model Merging**: Practice merging and deploying LoRA adapters\n",
    "6. **Memory Optimization**: Train a model with gradient checkpointing on limited GPU memory\n",
    "\n",
    "## \ud83c\udf89 Conclusion\n",
    "\n",
    "You've now mastered advanced fine-tuning techniques! Key achievements:\n",
    "- \u2705 Understanding PEFT methods and their trade-offs\n",
    "- \u2705 Implementing Unsloth for ultra-fast training\n",
    "- \u2705 Configuring LoRA and QLoRA adapters\n",
    "- \u2705 Preparing data for instruction tuning\n",
    "- \u2705 Advanced training techniques and optimizations\n",
    "- \u2705 Model evaluation and comparison\n",
    "- \u2705 Production deployment considerations\n",
    "\n",
    "Ready to move on to production deployment! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}