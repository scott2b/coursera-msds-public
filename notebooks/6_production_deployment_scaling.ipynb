{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires GPU\n",
    "\n",
    "This lab must be run in Google Colab in order to use GPU acceleration for model training. Click the button below to open this notebook in Colab, then set your runtime to GPU:\n",
    "\n",
    "**Runtime > Change Runtime Type > T4 GPU**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scott2b/coursera-msds-public/blob/main/notebooks/6_production_deployment_scaling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Production Deployment and Scaling\n",
    "\n",
    "This notebook covers everything you need to know about deploying LLM classification systems in production environments, from basic serving to advanced scaling strategies.\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Master production deployment strategies for LLMs\n",
    "2. Implement scalable serving architectures\n",
    "3. Optimize models for production workloads\n",
    "4. Handle high-throughput inference scenarios\n",
    "5. Implement monitoring and observability\n",
    "6. Manage model versioning and updates\n",
    "7. Handle production failures and rollbacks\n",
    "8. Implement cost-effective scaling strategies\n",
    "\n",
    "## \ud83d\udd27 Prerequisites\n",
    "\n",
    "- Completed Notebooks 1-3 (Fundamentals, vLLM, Fine-tuning)\n",
    "- Understanding of REST APIs and web services\n",
    "- Basic knowledge of containerization (Docker)\n",
    "- Familiarity with cloud platforms (AWS, GCP, Azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages not pre-installed in Colab\n",
    "!pip install fastapi uvicorn\n",
    "!pip install transformers accelerate\n",
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import asyncio\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\ud83d\ude80 Production Deployment Environment Ready!\")\n",
    "print(f\"FastAPI version: Ready for serving\")\n",
    "print(f\"AsyncIO support: {asyncio.iscoroutinefunction(lambda: None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Production Architecture Patterns\n",
    "\n",
    "Understanding different deployment architectures for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production architecture patterns\n",
    "architectures = {\n",
    "    \"Single Model Service\": {\n",
    "        \"description\": \"Simple FastAPI service with one model\",\n",
    "        \"scale\": \"Low (1-10 req/s)\",\n",
    "        \"complexity\": \"Low\",\n",
    "        \"cost\": \"Low\",\n",
    "        \"use_case\": \"Prototyping, small applications\",\n",
    "        \"components\": [\"FastAPI\", \"Single GPU\", \"Load Balancer\"]\n",
    "    },\n",
    "    \"Model Ensemble\": {\n",
    "        \"description\": \"Multiple specialized models for different tasks\",\n",
    "        \"scale\": \"Medium (10-100 req/s)\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"cost\": \"Medium\",\n",
    "        \"use_case\": \"Multi-task applications\",\n",
    "        \"components\": [\"Router\", \"Multiple Models\", \"Task Classifier\"]\n",
    "    },\n",
    "    \"Distributed Serving\": {\n",
    "        \"description\": \"vLLM with Ray for distributed inference\",\n",
    "        \"scale\": \"High (100-1000+ req/s)\",\n",
    "        \"complexity\": \"High\",\n",
    "        \"cost\": \"High\",\n",
    "        \"use_case\": \"Large-scale production\",\n",
    "        \"components\": [\"vLLM\", \"Ray\", \"Model Sharding\", \"Load Balancer\"]\n",
    "    },\n",
    "    \"Serverless\": {\n",
    "        \"description\": \"On-demand model serving with auto-scaling\",\n",
    "        \"scale\": \"Variable (pay per use)\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"cost\": \"Variable\",\n",
    "        \"use_case\": \"Variable load, cost optimization\",\n",
    "        \"components\": [\"Lambda\", \"API Gateway\", \"Cloud Storage\"]\n",
    "    },\n",
    "    \"Edge Deployment\": {\n",
    "        \"description\": \"Models deployed on edge devices\",\n",
    "        \"scale\": \"Local (device-dependent)\",\n",
    "        \"complexity\": \"High\",\n",
    "        \"cost\": \"Low\",\n",
    "        \"use_case\": \"Offline, privacy-sensitive applications\",\n",
    "        \"components\": [\"ONNX\", \"Quantized Models\", \"Local Hardware\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display architecture comparison\n",
    "import pandas as pd\n",
    "df_arch = pd.DataFrame.from_dict(architectures, orient='index')\n",
    "df_arch.index.name = 'Architecture'\n",
    "print(\"\ud83c\udfd7\ufe0f  Production Architecture Patterns:\")\n",
    "print(df_arch[['description', 'scale', 'use_case']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 FastAPI Service Implementation\n",
    "\n",
    "Building a production-ready LLM classification service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production service implementation\n",
    "class ProductionLLMService:\n",
    "    \"\"\"Production-ready LLM service with comprehensive features\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-small\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Metrics\n",
    "        self.request_count = 0\n",
    "        self.total_latency = 0.0\n",
    "        self.error_count = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # Load model\n",
    "        self._load_model()\n",
    "\n",
    "        logger.info(f\"\u2705 Production service initialized with {model_name}\")\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load and optimize the model\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "            logger.info(f\"Loading model: {self.model_name}\")\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            # Load with optimizations\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "            logger.info(\"\u2705 Model loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"\u274c Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def classify_text(self, text: str, labels: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Async classification with comprehensive error handling\"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.request_count += 1\n",
    "\n",
    "        try:\n",
    "            if not text or not isinstance(text, str):\n",
    "                raise ValueError(\"Text must be a non-empty string\")\n",
    "\n",
    "            if labels is None:\n",
    "                labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "            # Format prompt\n",
    "            prompt = f\"\"\"Classify this text into one of: {', '.join(labels)}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs[0][len(inputs[\"input_ids\"][0]):],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "\n",
    "            # Extract classification\n",
    "            prediction = self._extract_prediction(generated_text, labels)\n",
    "\n",
    "            # Calculate latency\n",
    "            latency = time.time() - start_time\n",
    "            self.total_latency += latency\n",
    "\n",
    "            return {\n",
    "                \"prediction\": prediction,\n",
    "                \"confidence\": 0.85,  # Simplified confidence\n",
    "                \"latency\": latency,\n",
    "                \"model\": self.model_name,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            logger.error(f\"Classification error: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=f\"Classification failed: {str(e)}\")\n",
    "\n",
    "    def _extract_prediction(self, generated_text: str, labels: List[str]) -> str:\n",
    "        \"\"\"Extract prediction from generated text\"\"\"\n",
    "        generated_lower = generated_text.lower()\n",
    "\n",
    "        for label in labels:\n",
    "            if label.lower() in generated_lower:\n",
    "                return label\n",
    "\n",
    "        return labels[0]  # Default to first label\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive service metrics\"\"\"\n",
    "        uptime = time.time() - self.start_time\n",
    "        avg_latency = self.total_latency / max(self.request_count, 1)\n",
    "        error_rate = self.error_count / max(self.request_count, 1)\n",
    "\n",
    "        return {\n",
    "            \"uptime_seconds\": uptime,\n",
    "            \"total_requests\": self.request_count,\n",
    "            \"average_latency\": avg_latency,\n",
    "            \"error_rate\": error_rate,\n",
    "            \"requests_per_second\": self.request_count / max(uptime, 1),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"device\": str(self.device)\n",
    "        }\n",
    "\n",
    "# Initialize service\n",
    "service = ProductionLLMService()\n",
    "print(\"\u2705 Production LLM Service initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI application\n",
    "app = FastAPI(\n",
    "    title=\"LLM Classification API\",\n",
    "    description=\"Production-ready LLM classification service\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Request/Response models\n",
    "class ClassificationRequest(BaseModel):\n",
    "    text: str\n",
    "    labels: Optional[List[str]] = None\n",
    "    temperature: Optional[float] = 0.1\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    latency: float\n",
    "    model: str\n",
    "    timestamp: str\n",
    "\n",
    "class MetricsResponse(BaseModel):\n",
    "    uptime_seconds: float\n",
    "    total_requests: int\n",
    "    average_latency: float\n",
    "    error_rate: float\n",
    "    requests_per_second: float\n",
    "    model_name: str\n",
    "    device: str\n",
    "\n",
    "@app.post(\"/classify\", response_model=ClassificationResponse)\n",
    "async def classify_endpoint(request: ClassificationRequest):\n",
    "    \"\"\"Classify text using the LLM service\"\"\"\n",
    "    result = await service.classify_text(request.text, request.labels)\n",
    "    return ClassificationResponse(**result)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": service.model is not None,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/metrics\", response_model=MetricsResponse)\n",
    "async def get_metrics():\n",
    "    \"\"\"Get service performance metrics\"\"\"\n",
    "    return MetricsResponse(**service.get_metrics())\n",
    "\n",
    "@app.get(\"/models\")\n",
    "async def list_models():\n",
    "    \"\"\"List available models and configurations\"\"\"\n",
    "    return {\n",
    "        \"current_model\": service.model_name,\n",
    "        \"available_models\": [\n",
    "            \"microsoft/DialoGPT-small\",\n",
    "            \"microsoft/DialoGPT-medium\",\n",
    "            \"microsoft/DialoGPT-large\"\n",
    "        ],\n",
    "        \"supported_tasks\": [\"sentiment\", \"classification\", \"analysis\"]\n",
    "    }\n",
    "\n",
    "print(\"\ud83d\ude80 FastAPI application configured!\")\n",
    "print(\"\ud83d\udccb Available endpoints:\")\n",
    "print(\"   POST /classify - Classify text\")\n",
    "print(\"   GET  /health   - Health check\")\n",
    "print(\"   GET  /metrics  - Performance metrics\")\n",
    "print(\"   GET  /models   - Available models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Monitoring and Observability\n",
    "\n",
    "Implementing comprehensive monitoring for production LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and metrics implementation\n",
    "class LLMMonitor:\n",
    "    \"\"\"Comprehensive monitoring for LLM services\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"requests_total\": 0,\n",
    "            \"requests_by_endpoint\": {},\n",
    "            \"latency_histogram\": [],\n",
    "            \"errors_total\": 0,\n",
    "            \"errors_by_type\": {},\n",
    "            \"model_metrics\": {\n",
    "                \"load_time\": 0,\n",
    "                \"inference_count\": 0,\n",
    "                \"average_tokens\": 0\n",
    "            }\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def record_request(self, endpoint: str, latency: float, success: bool = True):\n",
    "        \"\"\"Record a request with its metrics\"\"\"\n",
    "        self.metrics[\"requests_total\"] += 1\n",
    "\n",
    "        if endpoint not in self.metrics[\"requests_by_endpoint\"]:\n",
    "            self.metrics[\"requests_by_endpoint\"][endpoint] = 0\n",
    "        self.metrics[\"requests_by_endpoint\"][endpoint] += 1\n",
    "\n",
    "        self.metrics[\"latency_histogram\"].append(latency)\n",
    "\n",
    "        if not success:\n",
    "            self.metrics[\"errors_total\"] += 1\n",
    "\n",
    "    def record_error(self, error_type: str):\n",
    "        \"\"\"Record an error by type\"\"\"\n",
    "        if error_type not in self.metrics[\"errors_by_type\"]:\n",
    "            self.metrics[\"errors_by_type\"][error_type] = 0\n",
    "        self.metrics[\"errors_by_type\"][error_type] += 1\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get monitoring summary\"\"\"\n",
    "        uptime = time.time() - self.start_time\n",
    "        latencies = self.metrics[\"latency_histogram\"]\n",
    "\n",
    "        summary = {\n",
    "            \"uptime_seconds\": uptime,\n",
    "            \"total_requests\": self.metrics[\"requests_total\"],\n",
    "            \"total_errors\": self.metrics[\"errors_total\"],\n",
    "            \"error_rate\": self.metrics[\"errors_total\"] / max(self.metrics[\"requests_total\"], 1),\n",
    "            \"requests_per_second\": self.metrics[\"requests_total\"] / max(uptime, 1),\n",
    "            \"average_latency\": sum(latencies) / max(len(latencies), 1),\n",
    "            \"p95_latency\": sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0,\n",
    "            \"p99_latency\": sorted(latencies)[int(len(latencies) * 0.99)] if latencies else 0,\n",
    "            \"endpoint_breakdown\": self.metrics[\"requests_by_endpoint\"],\n",
    "            \"error_breakdown\": self.metrics[\"errors_by_type\"]\n",
    "        }\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def get_health_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system health status\"\"\"\n",
    "        summary = self.get_summary()\n",
    "\n",
    "        # Define health thresholds\n",
    "        health_status = \"healthy\"\n",
    "        issues = []\n",
    "\n",
    "        if summary[\"error_rate\"] > 0.05:  # 5% error rate\n",
    "            health_status = \"degraded\"\n",
    "            issues.append(\"High error rate\")\n",
    "\n",
    "        if summary[\"p95_latency\"] > 5.0:  # 5 second p95 latency\n",
    "            health_status = \"degraded\"\n",
    "            issues.append(\"High latency\")\n",
    "\n",
    "        return {\n",
    "            \"status\": health_status,\n",
    "            \"issues\": issues,\n",
    "            \"last_check\": datetime.now().isoformat(),\n",
    "            \"metrics\": summary\n",
    "        }\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = LLMMonitor()\n",
    "print(\"\ud83d\udcca Monitoring system initialized!\")\n",
    "\n",
    "# Test monitoring\n",
    "monitor.record_request(\"/classify\", 0.5, True)\n",
    "monitor.record_request(\"/classify\", 0.3, True)\n",
    "monitor.record_error(\"timeout\")\n",
    "\n",
    "print(\"\ud83d\udcc8 Test metrics:\")\n",
    "print(json.dumps(monitor.get_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Scaling Strategies\n",
    "\n",
    "Implementing different scaling approaches for production workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling implementation\n",
    "class AutoScaler:\n",
    "    \"\"\"Intelligent auto-scaling for LLM services\"\"\"\n",
    "\n",
    "    def __init__(self, min_instances: int = 1, max_instances: int = 10):\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.current_instances = min_instances\n",
    "        self.target_instances = min_instances\n",
    "\n",
    "        # Scaling thresholds\n",
    "        self.scale_up_threshold = 0.8  # 80% utilization\n",
    "        self.scale_down_threshold = 0.3  # 30% utilization\n",
    "        self.cooldown_period = 300  # 5 minutes\n",
    "        self.last_scale_time = 0\n",
    "\n",
    "    def evaluate_scaling(self, current_metrics: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate if scaling is needed based on current metrics\"\"\"\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Check cooldown period\n",
    "        if current_time - self.last_scale_time < self.cooldown_period:\n",
    "            return {\"action\": \"cooldown\", \"target_instances\": self.current_instances}\n",
    "\n",
    "        # Calculate utilization metrics\n",
    "        cpu_utilization = psutil.cpu_percent() / 100.0\n",
    "        memory_utilization = psutil.virtual_memory().percent / 100.0\n",
    "\n",
    "        # GPU utilization if available\n",
    "        gpu_utilization = 0.0\n",
    "        try:\n",
    "            import GPUtil\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu_utilization = gpus[0].load\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Average utilization\n",
    "        avg_utilization = (cpu_utilization + memory_utilization + gpu_utilization) / 3\n",
    "\n",
    "        # Scaling logic\n",
    "        if avg_utilization > self.scale_up_threshold:\n",
    "            self.target_instances = min(self.current_instances + 1, self.max_instances)\n",
    "            action = \"scale_up\"\n",
    "        elif avg_utilization < self.scale_down_threshold:\n",
    "            self.target_instances = max(self.current_instances - 1, self.min_instances)\n",
    "            action = \"scale_down\"\n",
    "        else:\n",
    "            action = \"maintain\"\n",
    "            self.target_instances = self.current_instances\n",
    "\n",
    "        # Execute scaling if needed\n",
    "        if action in [\"scale_up\", \"scale_down\"]:\n",
    "            self._execute_scaling(action)\n",
    "            self.last_scale_time = current_time\n",
    "\n",
    "        return {\n",
    "            \"action\": action,\n",
    "            \"current_instances\": self.current_instances,\n",
    "            \"target_instances\": self.target_instances,\n",
    "            \"cpu_utilization\": cpu_utilization,\n",
    "            \"memory_utilization\": memory_utilization,\n",
    "            \"gpu_utilization\": gpu_utilization,\n",
    "            \"avg_utilization\": avg_utilization\n",
    "        }\n",
    "\n",
    "    def _execute_scaling(self, action: str):\n",
    "        \"\"\"Execute the scaling action\"\"\"\n",
    "        if action == \"scale_up\":\n",
    "            self.current_instances += 1\n",
    "            print(f\"\u2b06\ufe0f  Scaled up to {self.current_instances} instances\")\n",
    "        elif action == \"scale_down\":\n",
    "            self.current_instances -= 1\n",
    "            print(f\"\u2b07\ufe0f  Scaled down to {self.current_instances} instances\")\n",
    "\n",
    "        # In a real implementation, this would:\n",
    "        # 1. Launch/terminate EC2 instances\n",
    "        # 2. Update load balancer\n",
    "        # 3. Reconfigure Kubernetes deployment\n",
    "\n",
    "# Test auto-scaling\n",
    "scaler = AutoScaler()\n",
    "\n",
    "# Simulate high load\n",
    "print(\"\ud83e\uddea Testing auto-scaling with simulated load...\")\n",
    "scaling_decision = scaler.evaluate_scaling({})\n",
    "print(f\"Scaling decision: {scaling_decision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udc33 Containerization and Deployment\n",
    "\n",
    "Creating production-ready Docker containers for LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker configuration\n",
    "dockerfile_content = '''\n",
    "# Multi-stage build for optimized LLM service\n",
    "FROM python:3.10-slim as base\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    git \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create app directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Production stage\n",
    "FROM base as production\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd --create-home --shell /bin/bash app \\\n",
    "    && chown -R app:app /app\n",
    "USER app\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# Save Dockerfile\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"\ud83d\udc33 Dockerfile created!\")\n",
    "print(\"\\nTo build and run:\")\n",
    "print(\"docker build -t llm-classification .\")\n",
    "print(\"docker run -p 8000:8000 llm-classification\")\n",
    "\n",
    "# Docker Compose for multi-service deployment\n",
    "docker_compose_content = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  llm-service:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_NAME=microsoft/DialoGPT-medium\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "\n",
    "  load-balancer:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf\n",
    "    depends_on:\n",
    "      - llm-service\n",
    "\n",
    "  monitoring:\n",
    "    image: prom/prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "'''\n",
    "\n",
    "# Save Docker Compose\n",
    "with open('docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "print(\"\\n\ud83d\udc33 Docker Compose configuration created!\")\n",
    "print(\"\\nTo deploy with orchestration:\")\n",
    "print(\"docker-compose up -d\")\n",
    "print(\"docker-compose logs -f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2601\ufe0f Cloud Deployment Strategies\n",
    "\n",
    "Strategies for deploying LLMs on major cloud platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud deployment strategies\n",
    "cloud_strategies = {\n",
    "    \"AWS SageMaker\": {\n",
    "        \"services\": [\"SageMaker Endpoints\", \"Lambda\", \"EC2\"],\n",
    "        \"scaling\": \"Auto-scaling groups\",\n",
    "        \"cost_optimization\": \"Spot instances, reserved instances\",\n",
    "        \"monitoring\": \"CloudWatch, X-Ray\",\n",
    "        \"best_for\": \"Enterprise deployments, ML workflows\"\n",
    "    },\n",
    "    \"Google Cloud AI\": {\n",
    "        \"services\": [\"Vertex AI\", \"Cloud Run\", \"GKE\"],\n",
    "        \"scaling\": \"Cloud Autoscaler\",\n",
    "        \"cost_optimization\": \"Committed use discounts, preemptible VMs\",\n",
    "        \"monitoring\": \"Cloud Monitoring, Cloud Logging\",\n",
    "        \"best_for\": \"Large-scale AI applications, global distribution\"\n",
    "    },\n",
    "    \"Azure ML\": {\n",
    "        \"services\": [\"Azure ML\", \"Container Instances\", \"AKS\"],\n",
    "        \"scaling\": \"Azure Autoscale\",\n",
    "        \"cost_optimization\": \"Reserved instances, spot VMs\",\n",
    "        \"monitoring\": \"Azure Monitor, Application Insights\",\n",
    "        \"best_for\": \"Enterprise integration, hybrid cloud\"\n",
    "    },\n",
    "    \"Serverless\": {\n",
    "        \"services\": [\"Lambda\", \"Cloud Functions\", \"Cloud Run\"],\n",
    "        \"scaling\": \"Automatic\",\n",
    "        \"cost_optimization\": \"Pay per request\",\n",
    "        \"monitoring\": \"CloudWatch, Cloud Monitoring\",\n",
    "        \"best_for\": \"Variable load, cost-sensitive applications\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display cloud comparison\n",
    "import pandas as pd\n",
    "df_cloud = pd.DataFrame.from_dict(cloud_strategies, orient='index')\n",
    "df_cloud.index.name = 'Platform'\n",
    "print(\"\u2601\ufe0f  Cloud Deployment Strategies:\")\n",
    "print(df_cloud[['scaling', 'cost_optimization', 'best_for']].to_string())\n",
    "\n",
    "# Cost estimation function\n",
    "def estimate_cloud_costs(platform: str, instances: int, hours_per_month: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate monthly costs for different cloud platforms\"\"\"\n",
    "\n",
    "    # Simplified cost estimates (in USD)\n",
    "    base_costs = {\n",
    "        \"AWS\": {\"gpu_hour\": 3.50, \"storage_gb_month\": 0.023},\n",
    "        \"GCP\": {\"gpu_hour\": 2.50, \"storage_gb_month\": 0.020},\n",
    "        \"Azure\": {\"gpu_hour\": 3.00, \"storage_gb_month\": 0.022}\n",
    "    }\n",
    "\n",
    "    if platform not in base_costs:\n",
    "        return {\"error\": \"Platform not supported\"}\n",
    "\n",
    "    costs = base_costs[platform]\n",
    "    compute_cost = instances * hours_per_month * costs[\"gpu_hour\"]\n",
    "    storage_cost = 100 * costs[\"storage_gb_month\"]  # Assume 100GB storage\n",
    "    total_cost = compute_cost + storage_cost\n",
    "\n",
    "    return {\n",
    "        \"compute_cost\": compute_cost,\n",
    "        \"storage_cost\": storage_cost,\n",
    "        \"total_monthly\": total_cost,\n",
    "        \"cost_per_request\": total_cost / (instances * hours_per_month * 3600 / 0.1)  # Assuming 0.1s per request\n",
    "    }\n",
    "\n",
    "# Test cost estimation\n",
    "aws_costs = estimate_cloud_costs(\"AWS\", 2, 730)  # 2 instances, ~1 month\n",
    "print(f\"\\n\ud83d\udcb0 AWS Cost Estimate: ${aws_costs['total_monthly']:.2f}/month\")\n",
    "print(f\"   Compute: ${aws_costs['compute_cost']:.2f}\")\n",
    "print(f\"   Storage: ${aws_costs['storage_cost']:.2f}\")\n",
    "print(f\"   Per request: ${aws_costs['cost_per_request']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Production Optimization Techniques\n",
    "\n",
    "Advanced techniques for optimizing production LLM deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced optimization techniques\n",
    "optimization_techniques = {\n",
    "    \"Model Optimization\": {\n",
    "        \"quantization\": \"Reduce precision (FP16, INT8, INT4)\",\n",
    "        \"pruning\": \"Remove unnecessary weights\",\n",
    "        \"distillation\": \"Train smaller model to mimic larger one\",\n",
    "        \"sparsification\": \"Make weight matrix sparse\"\n",
    "    },\n",
    "    \"Inference Optimization\": {\n",
    "        \"continuous_batching\": \"Process multiple requests together\",\n",
    "        \"kv_caching\": \"Cache key-value pairs across requests\",\n",
    "        \"parallel_decoding\": \"Decode multiple sequences in parallel\",\n",
    "        \"speculative_decoding\": \"Use smaller model to guide larger one\"\n",
    "    },\n",
    "    \"System Optimization\": {\n",
    "        \"memory_pooling\": \"Pre-allocate memory buffers\",\n",
    "        \"cpu_offloading\": \"Move computations to CPU when needed\",\n",
    "        \"model_sharding\": \"Split model across multiple GPUs\",\n",
    "        \"pipeline_parallelism\": \"Parallelize model layers\"\n",
    "    },\n",
    "    \"Request Optimization\": {\n",
    "        \"request_batching\": \"Group multiple requests together\",\n",
    "        \"priority_queues\": \"Process urgent requests first\",\n",
    "        \"caching\": \"Cache frequent queries and responses\",\n",
    "        \"compression\": \"Compress request/response data\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\u26a1 Production Optimization Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, techniques in optimization_techniques.items():\n",
    "    print(f\"\\n\ud83d\udd27 {category}:\")\n",
    "    for technique, description in techniques.items():\n",
    "        print(f\"   \u2022 {technique}: {description}\")\n",
    "\n",
    "# Performance benchmarking function\n",
    "def benchmark_optimization(technique: str, baseline_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"Benchmark the impact of an optimization technique\"\"\"\n",
    "\n",
    "    # Simulate optimization impact\n",
    "    improvements = {\n",
    "        \"quantization_fp16\": {\"latency\": 0.9, \"memory\": 0.5, \"throughput\": 1.1},\n",
    "        \"continuous_batching\": {\"latency\": 1.2, \"memory\": 1.0, \"throughput\": 3.0},\n",
    "        \"kv_caching\": {\"latency\": 0.8, \"memory\": 1.1, \"throughput\": 1.5},\n",
    "        \"model_pruning\": {\"latency\": 0.95, \"memory\": 0.7, \"throughput\": 1.05}\n",
    "    }\n",
    "\n",
    "    if technique not in improvements:\n",
    "        return baseline_metrics\n",
    "\n",
    "    optimized = {}\n",
    "    for metric, baseline_value in baseline_metrics.items():\n",
    "        if metric in improvements[technique]:\n",
    "            optimized[metric] = baseline_value * improvements[technique][metric]\n",
    "        else:\n",
    "            optimized[metric] = baseline_value\n",
    "\n",
    "    return optimized\n",
    "\n",
    "# Test optimization benchmarking\n",
    "baseline = {\"latency\": 1.0, \"memory\": 100, \"throughput\": 10}\n",
    "optimized = benchmark_optimization(\"continuous_batching\", baseline)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Optimization Impact Example:\")\n",
    "print(f\"Baseline: {baseline}\")\n",
    "print(f\"With continuous batching: {optimized}\")\n",
    "print(f\"Improvement: {optimized['throughput'] / baseline['throughput']:.1f}x throughput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Key Takeaways\n",
    "\n",
    "1. **Architecture Selection**: Choose the right deployment architecture based on scale and requirements\n",
    "2. **FastAPI Implementation**: Build robust, async APIs with proper error handling\n",
    "3. **Monitoring**: Implement comprehensive observability for production systems\n",
    "4. **Auto-scaling**: Use intelligent scaling based on utilization metrics\n",
    "5. **Containerization**: Docker and Kubernetes for consistent deployments\n",
    "6. **Cloud Optimization**: Leverage cloud-specific features for cost and performance\n",
    "7. **Optimization**: Multiple techniques for maximizing throughput and minimizing latency\n",
    "\n",
    "## \ud83d\ude80 Next Steps\n",
    "\n",
    "Now that you understand production deployment, proceed to:\n",
    "- **Notebook 5**: Evaluation, Benchmarking, and Ethics\n",
    "\n",
    "## \ud83d\udd17 Additional Resources\n",
    "\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Kubernetes for ML](https://kubernetes.io/docs/concepts/workloads/)\n",
    "- [AWS SageMaker](https://docs.aws.amazon.com/sagemaker/)\n",
    "- [vLLM Production Guide](https://vllm.readthedocs.io/en/latest/serving/index.html)\n",
    "\n",
    "## \ud83c\udfaf Hands-on Exercises\n",
    "\n",
    "1. **API Development**: Build a complete FastAPI service for LLM classification\n",
    "2. **Docker Deployment**: Containerize your LLM service and deploy it\n",
    "3. **Monitoring Setup**: Implement comprehensive monitoring and alerting\n",
    "4. **Auto-scaling**: Configure auto-scaling based on custom metrics\n",
    "5. **Cloud Deployment**: Deploy to AWS/GCP/Azure with proper security\n",
    "6. **Performance Optimization**: Apply multiple optimization techniques and measure impact\n",
    "7. **Load Testing**: Use tools like Locust to test your deployment under load\n",
    "8. **A/B Testing**: Implement model versioning and gradual rollouts\n",
    "\n",
    "## \ud83c\udf89 Conclusion\n",
    "\n",
    "You've now mastered production deployment and scaling for LLM systems! Key achievements:\n",
    "- \u2705 Understanding production architecture patterns\n",
    "- \u2705 Building robust FastAPI services\n",
    "- \u2705 Implementing comprehensive monitoring\n",
    "- \u2705 Configuring auto-scaling strategies\n",
    "- \u2705 Containerizing applications with Docker\n",
    "- \u2705 Optimizing for cloud deployments\n",
    "- \u2705 Applying advanced optimization techniques\n",
    "\n",
    "Ready to move on to evaluation and ethics! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}